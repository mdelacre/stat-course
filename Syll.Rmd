---
title: "Statistiques"
author: "Marie Delacre"
fig_caption: yes
output:
  word_document: 
    reference_docx: draft-styles.docx
  pdf_document: default
figurelist: no
figsintext: yes
email: marie.delacre@ulb.ac.be
---

```{r setup, include=FALSE}
library(knitr)
library(smoothmest)
knitr::opts_chunk$set(echo = TRUE)
```

```{r,include=FALSE}

# Plan de travail
chap_intro="chapitre 1" # qu'est-ce que la stat, et à quoi ça sert?
chap_variables="chapitre 2" # catégorisation des variables
chap_graph="chapitre 3" # exploration graphique des données
chap_algebre="chapitre 4" # exploration algébrique des données
chap_distributions="chapitre 5" # les distributions binomiales et normales

# Table and Figure list

# Chapitre 1
table1_brut="Table 1.1" # Données brutes récoltées sur un échantillon de n employés belges du secteur                               #automobile" , utilise les données data1_brut

# Chapitre 3
fumeur_freq = "Table 3.1"
table3.1_brut= "Table 3.2"     # Données brutes correspondant au nombre d'enfants par ménage
table3.1_freq= "Table 3.3"     # Données transnumérisées correspondant au nombre d'enfants par ménage
fumeur_degres = "Table 3.4"  # degrés pour le diagramme circulaire
data3.1_degres="Table 3.5"

table3.2_freq="Table 3.6"      # PIB par habitant et par pays
table3.3_freq_a="Table 3.7"    # âge des habitants (catégories homogènes)
table3.3_freq_b="Table 3.8"    # âge des habitants (catégories non homogènes)

table3.4_brut="Table 3.9"
table3.5_brut="Table 3.10"

table3.6_serie1="Table 3.11"
table3.6_serie2="Table 3.12"
table3.6_serie3="Table 3.13"

table3.7="Table 3.14"

barplot_fumeur_freq="Figure 3.1"
pie_fumeur_freq="Figure 3.2"
barplot_data3.1_freq="Figure 3.3" # barplot des fréquences de la variable nombre d'enfants
pie_data3.1_freq="Figure 3.4"     # pie chart des fréquences de la variable nombre d'enfants
cumdiag_data3.1_freq="Figure 3.5"
stair_data3.1_freq="Figure 3.6"   # diagramme en escalier des fréquences de la variable nombre d'enfants
barplot_data3.2_freq="Figure 3.7"  
barplot_data3.3_freq_a="Figure 3.8"   # diagramme en escalier des fréquences de la variable nombre d'enfants
barplot_data3.3_freq_b="Figure 3.9"   # diagramme en escalier des fréquences de la variable nombre d'enfants
polygone_data3.2_freq="Figure 3.10"  
Figuredistribution="Figure 3.11"
polygone_data3.3_freq_a="Figure 3.12"   # diagramme en escalier des fréquences de la variable nombre d'enfants
polygone_data3.3_freq_b="Figure 3.13"   # diagramme en escalier des fréquences de la variable nombre d'enfants
boxplot_serie1="Figure 3.14"

# Chapitre 4
table4.1="Table 4.1" # extrait de la Table 1.1
table4.2="Table 4.2"  # extrait de la Table 1.1
notes_compta="Table 4.3" # données pour illustrer la variance
notes_ecart_math="Table 4.4"
notes_ecart_francais="Table 4.5"
notes_compta2="Table 4.6"

Figure4.1="Figure 4.1"
Figure4.2="Figure 4.2"
Figure4.3="Figure 4.3"
Figure4.4="Figure 4.4"

correlation_graph_a= "Figure 5.1"
correlation_graph_b= "Figure 5.2"
correlation_graph_geo= "Figure 5.3"
correlation_graph_agls= "Figure 5.4"
```
###### Pagebreak
# Chapitre 1: qu'est-ce que la statistique, et à quoi sert-elle?

## Qu'est-ce que la statistique? 

```{r echo=FALSE, results='asis'}
n=15
id <-1:n
genre<-c(0,0,1,0,0,0,1,1,1,0,0,1,1,0,1)
genre[genre==0]="masculin"
genre[genre==1]="féminin"
age<-c(19,25,21,27,30,32,30,19,21,21,24,27,26,30,26)
anciennete <-c(1,5,2,8,8,9,9,1,1,1,8,8,7,10,8)
langue <-c(3,1,1,2,1,2,3,2,2,1,1,2,2,2,2)
langue[langue==1]="français"
langue[langue==2]="néerlandais"
langue[langue==3]="allemand"
data1_brut=data.frame(id,genre,age,anciennete,langue)
```

La statistique est l'ensemble des instruments de recherches mathématiques qui permettent de **récolter**, **traiter** et **interpréter** un ensemble de données (généralement vaste). 

### La récolte des données: les concepts de population, individus et échantillons 

La **population** est l'ensemble des éléments (individus ou objets) auxquels on s'intéresse. Les éléments qui constitue cette population sont appelés **individus** ou **unité statistique**. Admettons par exemple que l'on souhaite étudier les caractéristiques de l'ensemble des employés belges du secteur automobile. Chaque employé de nationalité belge travaillant dans ce secteur constituera une unité statistique, et leur ensemble constituera la population d'intérêt.   

Il est bien souvent compliqué, voire impossible de recueillir des informations pour l'entièreté de la population tant celle-ci peut être vaste. Pour cette raison, on prélèvera généralement, aléatoirement, une partie de taille gérable de cette population que l'on appelle l'**échantillon**, et l'on mesurera, pour chaque individu de l'échantillon, un ensemble d'informations (des **variables**) qui nous intéresse, comme le genre, la situation géographique ou linguistique ou encore l'âge et le salaire moyen. 

Les variables que l'on récolte au sein de l'échantillon peuvent être de diverses natures. Autrement dit, il existe plusieurs manières de les mesurer. Imaginez par exemple que je vous demande d'estimer la silhouette d'un individu. Vous pourriez prendre un mêtre et me dire que son tour de taille est de 72 cm. Vous pourriez également vous contenter de dire qu'il est de taille "Medium". Décider de la manière dont on mesure les variables aura des implications importantes en termes de possibilités d'analyses et ne doit pas être laissé au hasard. Un chapitre ultérieur sera dédié aux variables et à leurs natures possibles (`r chap_variables`). 

### Le traitement des données, appelée également la statistique descriptive

Après avoir récolté des données, on se retrouve avec une série de variables pour chaque individu. Ces données pourront être encodées dans une base de données tel qu'illustré à la `r table1_brut`. Cette table est une représentation fictive de données socio-démographiques récoltées auprès de  `r length(data1_brut$id)` employés du secteur automobile. 

###### Pagebreak  
`r table1_brut`  
_Données socio-démographiques récoltées auprès de `r length(data1_brut$id)` employés du secteur automobile_
```{r echo=FALSE, results='asis'}
kable(data1_brut,format="markdown",align="c",col.names = c("i","genre ($W_i$)","âge ($X_i$)"," anciennete ($Y_i$)","langue ($Z_i$)"))
```

Notez que chaque ligne de la `r table1_brut` représente un individu de l'échantillon, et que chaque colonne représente une variable. C'est de la sorte que l'on représente généralement les données brutes (soit les données qui ont été récoltées directement auprès des participants) dans les logiciels de traitement de données. Par ailleurs, nous prendrons également l'habitude d'utiliser certaines notations. On symbolise généralement les variables par des lettres majuscules choisies à la fin de l'alphabet (par exemple, X, Y, Z...). On utilise l'indice _i_ pour se référer à un individu quelconque de l'échantillon. On symbolise par $X_i$ (X indicé i) la valeur d'un individu i sur la variable X, par $Y_i$ (Y indicé i) la valeur d'un individu i sur la variable Y, etc. Par exemple, l'ancienneté du `r data1_brut$id[3]`ème employé de l'échantillon est symbolisée par $Y_3$ = `r data1_brut$anciennete[3]`, l'âge du 
`r data1_brut$id[8]`ème employé de l'échantillon est symbolisée par $X_8$=`r data1_brut$age[8]`, etc. Gardez tout cela à l'esprit pour la suite de ce cours! 
Les données brutes sont généralement peu parlantes en elles-mêmes. Afin de pouvoir faire passer un message à travers ces données, il sera nécessaire de les simplifier, de les résumer. C'est l'objectif de la **statistique descriptive**. 

L'une des manières de présenter avantageusement les données est de se centrer sur une approche visuelle, soit la réalisation de graphique. Cela fera l'objet du `r chap_graph`. Au delà de la représentation graphique des données, plusieurs caractéristiques sont importantes à déterminer algébriquement. On peut par exemple se demander quel est l'âge moyen des participants de l'échantillon, ou encore quelle est la proportion de femmes au sein de celui-ci. Cela fera l'objet du `r chap_algebre`. 

Nous avons déjà évoqué le fait que la _nature_ des variables en jeu aura des implications sur les possibilités d'analyses. Le choix des représentations graphiques et des résumés algébriques que l'on pourra réaliser à l'aide des données dépendront de cette nature!

### L'interprétation des données, appelée également la statistique inférentielle

Il est très important de bien comprendre que la simple description d'un échantillon ne permet pas en tant que telle de tirer des conclusions générales sur l'ensemble de la population. Or, s'il est très intéressant de connaitre les caractéristiques de l'échantillon que l'on étudie, il sera souvent plus intéressant encore de pouvoir inférer les caractéristiques de notre échantillon à la population toute entière. Comment s'assurer que la proportion de femmes observées dans notre échantillon soit bien conforme à la proportion de femmes au sein de la population? Cela recquiert de faire de la **statistique inférentielle**. Cette forme de statistique ne fait pas partie des objectifs de ce cours. Sachez cependant qu'elle existe.

###### Pagebreak

# Chapitre 2: mesure des variables

Mesurer une variable correspond à attribuer un code, le plus souvent chiffré, à chaque individu. Le nom "variable" vient du fait que ce code peut varier d'un individu à l'autre, chaque code possible étant appelé une **modalité** de la variable.

Nous distinguerons les variables **qualitatives** et les variables **quantitatives**.

### Les variables qualitatives

Les variables **qualitatives** sont les variables qui permettent de distinguer les individus sur base de la catégorie à laquelle ils appartiennent. 

On parle de variables _**nominales**_ lorsqu'il n'est pas possible d'établir un ordre logique croissant ou décroissant entre les modalités de la variable qualitative. Par exemple, la variable "genre", qui permet de distinguer les hommes des femmes, est de type nominal. 

Bien qu'on leur attribue parfois des codes numériques, effectuer des opérations algébriques sur les valeurs de ces variables n'aurait pas de sens. Si je conviens arbitrairement d'attribuer le code "1" à tous les individus du genre masculin, et le code "2" à tous les individus du genre féminin, je ne fais qu'établir une convention sans aucun sens mathématique. Je pourrais tout aussi bien décider que le genre féminin soit codé "1" et que le genre masculin soit codé "4". Il n'y a aucun lien arithmétique entre ces classes. La valeur "2" n'a par exemple pas le statut de "double de la valeur 1", elle n'est même pas "plus grande que la valeur 1", elle est juste différente qualitativement. 

On parle de variables _**ordinales**_ lorsqu'il est possible d'établir un ordre logique croissant ou décroissant entre les modalités de la variable qualitative. Par exemple, une variable "taille" qui permet de distinguer trois groupes d'enfants suivant qu'ils soient petits, moyens ou grand est de type ordinal, dans la mesure où l'on sait que les grands sont plus grands que les moyens, étant eux-mêmes plus grands que les petits. 

Cette fois, si l'on décide d'attribuer des codes numériques à ces variables, il importera que l'ordination des chiffres reflète la relation entre les catégories: les codes attribués aux catégories basses devront être plus petits que ceux attribuées aux catégories plus élevées. Par exemple, on attribuera le code "1" aux petits, "2" aux moyens et "3" aux grands. Pourtant, il n'est toujours pas possible d'effectuer des opérations algébriques sur les valeurs de ces variables. En effet, si l'on peut déduire rapidement sur base des codes qu'un enfant ayant la valeur "3" sur la variable taille est plus grand qu'un enfant ayant la valeur "1", cela ne signifie pas pour autant qu'il est trois fois plus grand que ce dernier.  

## Les variables quantitatives

Les variables **quantitatives** sont les variables dont les valeurs sont intrinsèquement numériques, c'est-à-dire qu'il fait sens d'effectuer des opérations mathématiques sur ses valeurs. Par exemple, si un fermier dispose de deux poules, que la première pond trois oeufs et la deuxième en pond deux, il est correct de dire que le fermier trouvera 5 oeufs lorsqu'il se rendra dans son poulailler.

Les variables _**discrètes**_  ne peuvent prendre que des valeurs isolées et généralement entières, dans un intervalle de valeurs spécifiques dans R. Ce sont typiquement les variables de comptage (le nombre d'enfants dans une école, le nombre de filles au sein d'une famille...)

Les variables _**continues**_, au contraire, peuvent prendre (au moins théoriquement) n'importe quelle valeur numérique possible (soit une infinité de valeurs), dans un intervalle de valeurs spécifiques dans R. L'âge, les distance, le temps... sont des exemples de variables continues.

###### Pagebreak

# Chapitre 3: exploration graphique des données à une dimension

Les représentations graphiques sont généralement très appréciées, parce que lorsqu'elles sont réalisées judicieusement, elles permettent en un coup d'oeil de se faire une idée de la tendance principale des données. 

Il existe une multitude de représentations graphiques si bien qu'il n'est pas toujours évident de savoir quelle représentation choisir. Par ailleurs, certains logiciels proposent des solutions plus ou moins fantaisistes qui peuvent sembler tententes (représentations en 3D, avec couleurs inhabituelles, textures bois...). Je déconseille vivement l'usage de ces artifices. Gardez qu'une bonne présentation est la clé d'une bonne communication. Afin de vous guider vers des choix judicieux, je concluerai ce chapitre sur l'exploration des graphique par quelques conseils qui m'avaient été donnés lors d'un atelier, il y a quelques années, par un brillant scientifique de l'UCL dénommé Christian Ritter. 

Pour l'heure, rappelons deux éléments importants à garder à l'esprit lorsqu'on réalise un graphique. Premièrement, toutes les formes de graphiques ne conviennent pas à toutes les formes de données. Le choix du graphique devra dépendre de la nature des variables (on ne représentera pas de la même façon une variable discrète et une variable continue, par exemple). Il faudra donc bien comprendre la nature des variables que l'on souhaite représenter. Deuxièmement, un graphique constitue une représentation **simplifiée** des données. Cela signifie qu'on ne va pas y représenter directement les données brutes. Après avoir identifié la nature de la variable, la réalisation du graphique se déroulera donc en deux étapes: 1) simplification des données et 2) réalisation du graphique en tant que tel. 

Nous développerons deux catégories de graphiques: ceux qui sont construits sur base de **tableaux de fréquence** et ceux qui reposent sur le calcul de **quantiles**.
 
## Les graphiques construits sur base des tableaux de fréquences

### Variables qualitatives

#### Simplification des données

Imaginez que 40 participants aient répondu par "oui" ou "non" à la question suivante "Etes vous fumeur?" et que vous vous retrouvez avec l'ensemble de réponse suivant:

```{r echo=FALSE, results='asis'}
n_fumeur=40
fumeur=c("non","oui","non","non","non","non","non","non","non","oui","oui","non","oui","oui","oui","non","non","oui","oui","non","non","non","non","non","oui","non","non","non","non","oui","non","non","non","non","non","non","non","non","non","non")
Table=matrix(fumeur,4,n_fumeur/4)
kable(Table)
```

C'est ce qu'on appelle les données brutes. Une manière de résumer cet ensemble d'information est de calculer un **tableau de fréquences**, tel que représenté dans la `r fumeur_freq`. 

`r fumeur_freq`
_Transformation des données brutes relatives aux fumeurs en tableau de fréquence_
```{r echo=FALSE, results='asis'}
names_fumeur=names(table(fumeur))
fumeur[fumeur==1]="oui"
fumeur[fumeur==0]="non"
abs_fumeur=c(length(fumeur[fumeur=="non"]),length(fumeur[fumeur=="oui"]))
rel_fumeur=abs_fumeur/length(fumeur)
fumeur_tab=data.frame(names_fumeur,abs_fumeur,rel_fumeur)
kable(fumeur_tab,col.names=c("fumeur ($x_{j})$","$n_{j}$","$f_{j}$"))
```

Il s'agit d'un tableau dans lequel chaque ligne représente une valeur possible de la variable. Comme vous pouvez l'observer, deux lignes ont été envisagées (une pour les fumeurs, une pour les non fumeurs) et le nombre de personnes appartenant à chaque catégorie constitue ce que l'on appelle la **fréquence absolue**. La somme des fréquences absolues correspond au nombre total de personnes constituant l'échantillon (ici, n vaut `r sum(fumeur_tab$abs_fumeur)`). 

$$n=\sum_{j=1}^k n_j = `r abs_fumeur[1]`+`r abs_fumeur[2]` = `r sum(abs_fumeur)`$$ 
La colonne suivante contient la même information mais par rapport à l'ensemble des ménages, c'est la **fréquence relative**. 

$$f_j=\frac {n_j}{n}$$       
Par exemple, `r abs_fumeur[1]` personnes de l'échantillon sur `r sum(fumeur_tab$abs_fumeur)` ne fument pas, or `r abs_fumeur[1]`/`r sum(abs_fumeur)` x 100 = `r round(abs_fumeur[1]/sum(abs_fumeur)*100,2)` %   (`r round(abs_fumeur[1]/sum(abs_fumeur)*100,2)`% des personnes ne notre échantillon ne fument pas).

De même que pour les données brutes, nous prendrons l'habitude d'utiliser certaines notations. Dans un tableau de fréquence, on symbolise généralement l'ensemble des valeurs que peut prendre la variable par une lettre minuscule (Par exemple, les différentes valeurs que peut prendre la vaiable X seront symboliées par x). On utilise l'indice _j_ pour se référer à chaque valeur possible, j pouvant varier de 1 à k (dans le cas présent, k = `r length(names_fumeur)`). On symbolise par $x_j$ (x indicé j) la jème valeur possible de la variable X. Par exemple, si l'on choisit de nommer X la variable "fumer", $x_2=`r names_fumeur[2]`$. $n_j$ correspond aux fréquences absolues associées à chaque valeur $x_j$. Par exemple, $n_1$= `r abs_fumeur[1]`. Enfin, $f_i$ correspond aux fréquences relatives. Par exemple $f_2$= `r rel_fumeur[2]`. 

Notons que quand on étudie des variables nominales, peu importe l'ordre d'apparition des modalités de la variable dans le tableau de fréquence. Dans l'exemple, nous avons représenté d'abord les non fumeurs, suivi des fumeurs. L'inverse aurait pu être fait également sans pour autant altérer le contenu informatif du tableau. 

#### Réalisation graphique

##### Diagramme en bâtons

Le diagramme en bâtons consiste à représenter en abcisse les différentes valeurs que peut prendre la variable que l'on étudie($x_j$) et en ordonnée les fréquences associées à chaque valeur $x_j$ (soit $n_j$, soit $f_j$). Pour chaque valeur $x_j$, on trace un bâtonnet dont la hauteur varie en fonction de la fréquence associée à cette valeur $x_j$.

L'ordre dans lequel les différents bâtonnets importe peu lorsqu'on représente des variables nominales. De plus, étant donné que l'axe des abcisses n'est pas normé, la largeur des bâtons importe peu également (même si pour le confort des yeux, on les gardera généralement uniformes). 

La figure `r barplot_fumeur_freq`  montre deux diagrammes en bâtons de notre distribution. Que l'on représente les fréquences absolues ou les fréquences relatives sur l'axe des ordonnées importe peu en termes de forme de graphe. En revanche,  l'information donnée est un petit peu différente: dans le premier cas, on peut dénombrer le nombre de fumeur et de non fumeur, dans le second cas, on peut déterminer le pourcentage de personnse qui fument ou ne fument pas dans l'échantillon.

`r barplot_fumeur_freq`  
_Diagramme en barre de la distribution de la `r fumeur_freq`_
```{r barplot_fumeur, echo=FALSE}
par(mfrow=c(1,2))
barplot(table(fumeur),yaxt = "n",ylim=c(0,30),main="Fait d'être ou non fumeur",cex.main=.9,col="lightblue",ylab="Fréquence absolue")
axis(2, at = seq(0,30,2), labels = seq(0,30,2), las = 1)

barplot(table(fumeur)/length(fumeur)*100,yaxt = "n",ylim=c(0,75),main="Fait d'être ou non fumeur",cex.main=.9, col="lightblue",ylab="Fréquence relative (en %)")
axis(2, at = seq(0,75,5), labels = seq(0,75,5), las = 1)
```

##### Diagramme circulaire

On trouve occasionnellement dans les rapports des diagrammes circulaires (qu'on appelle aussi graphiques en camembert) pour représenter les variables.  Réaliser un diagramme circulaire consiste à subdiviser un cercle en autant de part qu'il n'y a de modalités possibles, chaque part ayant une taille proportionnelle à la fréquence associée à la modalité qu'elle représente.

La `r pie_fumeur_freq` montre deux diagrammes circulaires de notre distribution. De même que pour les diagrammes en bâtons, que ce soient les fréquences absolues ou les fréquences relatives importe peu de termes de forme de graphe, mais l'informatin donnée varie légèrement: dans le premier cas, on peut dénombrer le nombre de fumeur et de non fumeur, dans le second cas, on peut déterminer le pourcentage de personnse qui fument ou ne fument pas dans l'échantillon.

`r pie_fumeur_freq`
_Diagramme circulaire de la distribution de la `r table3.1_freq`_
```{r pie_fumeur, echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,2,2),xpd=FALSE)
col_pie=colorRampPalette(c("grey", "black"))(length(names_fumeur))
pie(table(fumeur),labels=abs_fumeur,cex.main=.9,cex.lab=.9,main="Fait d'être ou non fumeur",col=col_pie)
legend("top", legend=c("oui", "non"),col=c("black", "grey"), pch=c(19,19), cex=0.8,box.lty=0)
pie(table(fumeur)/length(fumeur)*100,labels=paste(round(rel_fumeur,2),"%"),cex.main=.9,cex.lab=.9,main="Fait d'être ou non fumeur",col=col_pie)
legend("top", legend=c("oui", "non"),col=c("black", "grey"), pch=c(19,19), cex=0.8,box.lty=0)
```

Déterminer l'angle pour chaque catégorie peut se faire en procédant à une règle de trois. Un cercle fait exactement 360° et doit représenter l'ensemble des individus constituant l'échantillon, soit `r length(fumeur)`. Ces 360° doivent être répartis entre les catégories, de manière proportionnelle à la fréquence de chaque catégorie. Par exemple, `r abs_fumeur[1]` personnes ne fument pas (valeur $x_j$ = `r names_fumeur[1]`).  Cela représente `r abs_fumeur[1]/length(fumeur)*100`% de l'échantillon, l'angle correspondra donc à `r abs_fumeur[1]/length(fumeur)*100`% des 360°. 
                                                                          
$$`r length(fumeur)` \longrightarrow 360°$$
$$\Longleftrightarrow 1 \longrightarrow \frac{360°}{`r length(fumeur)`}=`r 360/length(fumeur)`°$$
$$\Longleftrightarrow `r abs_fumeur[1]` \longrightarrow \frac{360°}{`r length(fumeur)`} \times `r abs_fumeur[1]`=`r 360/length(fumeur)*abs_fumeur[1]`°$$

Naturellement, dans la mesure où les parts sont de taille proportionnelle à la fréquence associée à chaque catégorie, déterminer l'angle pour chaque catégorie en utilisant les fréquences relatives, plutét que les fréquences absolues, fournit exactement le même résultat, à condition d'utiliser les valeurs exactes (non arrondies).  La preuve: 
                    
$$ 100 \% \longrightarrow 360°$$
$$\Longleftrightarrow 1\% \longrightarrow \frac{360°}{100}=`r 360/100`°$$
$$\Longleftrightarrow `r rel_fumeur[1]*100` \% \longrightarrow \frac{360°}{100} \times `r rel_fumeur[1]*100`=`r 360/100*rel_fumeur[1]*100`°$$

Bien que ça ne soit pas le cas dans cet exemple, les fréquences relatives contiendront parfois un nombre important (voire infini) de décimales. Il est dès lors généralement plus facile de déterminer les angles de chaque part sur base des fréquences abolues plutôt que sur base des fréquences relatives. La `r  fumeur_degres` fournit les angles qui devraient être associés à chaque catégorie dans l'exemple de la `r fumeur_freq`:
                                                                          
```{r echo=FALSE, results='asis'} 
angles_fumeurs=360/length(fumeur)*abs_fumeur
fumeur_deg=data.frame(names_fumeur,abs_fumeur,angles_fumeurs)
```

`r fumeur_degres`  
_Angle de chaque part du diagramme circulaire représententant les données de la `r fumeur_freq`_
```{r echo=FALSE, results='asis'}
kable(fumeur_deg,format="markdown",align="c",col.names=c("$x_j$","$n_j$","angle (en °)"))
```

Ce type de graphique présente de nombreux inconvénients en termes de communication. Entre autres, ils sont peu précis: lorsque deux catégories sont associées à des fréquences relativement similaires, il devient très difficile de repérer leur différence. Cela sera illustré dans la section dédiée aux diagrammes circulaires pour représenter des variables quantitatives discrètes. 

### Variables quantitatives discrètes 

```{r echo=FALSE, results='asis'}
n=40
id <-1:n
Enfant<-c(5,2,2,3,1,2,1,2,2,2,1,3,3,1,4,3,3,1,2,1,1,1,2,2,1,2,3,1,3,4,1,1,2,2,1,2,3,1,3,4)
data3.1_brut=data.frame(id,Enfant)

# découpe en vue de l'affichage graphique
affich_col=4
Num_gr=matrix(0,n/affich_col,affich_col) 
Enf_gr=matrix(0,n/affich_col,affich_col)
affich_tab3.1=matrix(0,n/affich_col,affich_col*2)

for(j in 1:affich_col){
  Num_gr[,j]=((n/affich_col)*(j-1)+1):(n/affich_col*j)
  Enf_gr[,j]=Enfant[((n/affich_col)*(j-1)+1):(n/affich_col*j)]
  affich_tab3.1[,((3*j)-(j+1)):(2*j)]=cbind(Num_gr[,j],Enf_gr[,j])
  }
```

`r table3.1_brut`  
_Données brutes relatives au nombre d'enfants par ménages, au sein de `r length(data3.1_brut$id)` ménages_
```{r echo=FALSE, results='asis'}
kable(affich_tab3.1,format="markdown",align="c",col.names=rep(c("i","Enfants ($X_i$)"),affich_col))
```

#### Simplification des données: transformation en tableaux de fréquences

```{r echo=FALSE, results='asis'}
freq_data3.1=table(Enfant,deparse.level=2)
names_data3.1=names(freq_data3.1)
abs_data3.1=tabulate(Enfant)
rel_data3.1=abs_data3.1/sum(freq_data3.1)*100

abscum_data3.1=cumsum(table(Enfant))
relcum_data3.1=cumsum(table(Enfant))/length(Enfant)*100

data3.1_freq=data.frame(names_data3.1,abs_data3.1,round(rel_data3.1,2),abscum_data3.1,round(relcum_data3.1,2)) 
```

La `r table3.1_brut` répertorie le nombre d'enfants vivant au sein de `r length(data3.1_brut$id)` ménages fictifs. Notez que j'ai représenté ces données brutes sur plusieurs colonnes distinctes par soucis d'économie de place. Cependant, dans un logiciel (excel, ,par exemple), il conviendrait de n'encoder que deux colonnes: une première relative au numéro (ou id) attribué à chaque sujet, et une deuxième relative à la variable "nombre d'enfants". Rappelez-vous: une ligne par sujet, et une colonne par variable!

La `r table3.1_freq` constitue le tableau de fréquence de ces données.

###### Pagebreak

`r table3.1_freq`  
_Transformation de la `r table3.1_brut` en tableau de fréquence_
```{r echo=FALSE, results='asis'}
kable(data3.1_freq,format="markdown",align="c",col.names=c("nombre d'enfants ($x_j$)","$n_j$","$f_j$ (en %)","$N_j$","$F_j$ (en %)"))
```
 
Comme vous pouvez l'observer, de même que fait précédemment pour les variables nominales, les nombres possibles d'enfants par ménage ont été envisagés et on leur associe  une fréquence absolue ($n_j$), et une fréquence relative ($f_j$). Par ailleurs, la somme des fréquences absolues correspond toujours au nombre total de ménages constituant l'échantillon (ici, n vaut `r sum(data3.1_freq$abs_data3.1)`). 

```{r echo=FALSE, results='asis'}
chaine=function(variable){
rep=NULL
for (i in 1:length(variable)){
      if (i==1){rep=paste(rep,variable[i])
      }else {
      if (variable[i]>=0){rep=paste(rep,"+",variable[i])}
      else {rep=paste(rep,variable[i])}}
   }
return(rep) 
}
```

$$n=\sum_{j=1}^k n_j = `r chaine(abs_data3.1)` = `r sum(abs_data3.1)`$$ 

Contrairement au cas des variables nominales, l'ordre de présentation de chaque ligne a une importance. On triera généralement les valeurs possibles de la variable représentée par ordre croissant (`r names_data3.1[1]` apparait avant `r names_data3.1[2]`, `r names_data3.1[2]` apparait avant `r names_data3.1[3]`, etc.). 

De plus, on pourra calculer ce qu'on appelle les **fréquences cumulées**, soit le cumul de plusieurs fréquences. 

Les fréquences absolues cumulées sont la somme de plusieurs fréquences absolues (notation: $N_i$).  

$$N_j=\sum_{j=1}^j \frac {n_j}{n},  1 \le j \le k$$

Cela permet de déterminer rapidement, par exemple, que `r abscum_data3.1[4]` ménages sont constitués de maximum `r names_data3.1[4]` enfants. Enfin,  cette information peut être exprimée par rapport à l'ensemble des ménages. On dira alors que `r round(relcum_data3.1[4],2)` % des ménages sont constitués de maximum `r names_data3.1[4]` enfants. Cette dernière expression correspond aux  **fréquences relatives cumulées**, soit la dernière colonne du tableau (notation: $F_i$). 

$$F_j=\sum_{j=1}^j f_j=\sum_{j=1}^j \frac {n_j}{n},1 \le j \le k $$   

#### Représentation graphique

##### Diagramme en bâtons

Dans la mesure où il existe un ordre logique entre les différentes valeurs de la variable représentée, l'ordre des bâtonnets importe (contrairement au cas des variables nominales). 

La `r barplot_data3.1_freq` montre deux diagrammes en bâtons de notre distribution. On constate à nouveau que l'allure générale du graphique est la même, que l'on représente les fréquences absolues ou relatives sur le diagramme. En revanche, l'information donnée est un petit peu différente: dans le premier cas, on peut dénombrer le nombre de ménages qui ont un nombre donné d'enfants, dans le second cas, on peut déterminer le pourcentage de ménages qui ont ce nombre d'enfants.

`r barplot_data3.1_freq`  
_Diagramme en barre de la distribution de la `r table3.1_freq`_
```{r barplot_data3.1, echo=FALSE}
par(mfrow=c(1,2))
barplot(table(data3.1_brut$Enfant),yaxt = "n",ylim=c(0,15),main="Nombre d'enfants par ménages",cex.main=.9,col="lightblue",ylab="Fréquence absolue")
axis(2, at = seq(0,14,2), labels = seq(0,14,2), las = 1)

barplot(table(data3.1_brut$Enfant)/length(data3.1_brut$id)*100,yaxt = "n",ylim=c(0,35),main="Nombre d'enfants par ménages",cex.main=.9, col="lightblue",ylab="Fréquence relative (en %)")
axis(2, at = seq(0,50,5), labels = seq(0,50,5), las = 1)
```

##### Diagramme circulaire

La `r pie_data3.1_freq` montre deux diagrammes circulaires de notre distribution: l'un qui représente les fréquences absolues, et l'autre qui représente les fréquences relatives. 

`r pie_data3.1_freq`
_Diagramme circulaire de la distribution de la `r table3.1_freq`_
```{r pie_data3.1, echo=FALSE}
par(mfrow=c(1,2),mar=c(1,2,2,2))
col_pie=colorRampPalette(c("lightgrey","black"))(length(names_data3.1))
pie(table(data3.1_brut$Enfant),labels=names_data3.1,cex.main=.9,cex.lab=.9,main="Nombre d'enfants par ménages",col=col_pie)
legend("top", legend=names_data3.1,col=col_pie, pch=c(19,19), cex=0.8,box.lty=0,,horiz=T)

pie(table(data3.1_brut$Enfant)/length(data3.1_brut$id)*100,labels=paste(round(rel_data3.1,2),"%"),cex.main=.9,cex.lab=.9,main="Nombre d'enfants par ménages",col=col_pie)
legend("top", legend=names_data3.1,col=col_pie, pch=c(19,19), cex=0.8,box.lty=0,horiz=T)
```
                
La `r  data3.1_degres` fournit les angles qui devraient être associés à chaque catégorie dans l'exemple de la `r table3.1_freq`. Ceux-ci ont été déterminés en procédant à une règle de trois, comme expliqué dans la section dédiée aux diagrammes circulaires pour représenter des variables nominales. 
                                                                          
```{r echo=FALSE, results='asis'} 
angles_data3.1=360/length(data3.1_brut$id)*abs_data3.1
data3.1_deg=data.frame(names_data3.1,abs_data3.1,angles_data3.1)
```

`r data3.1_degres`  
_Angle de chaque part du diagramme circulaire représententant les données de la `r table3.1_freq`_
```{r echo=FALSE, results='asis'}
kable(data3.1_deg,format="markdown",align="c",col.names=c("$x_j$","$n_j$","angle (en °)")) 
```

Il a déjà été précisé ce type de graphique est peu précis et que lorsque deux catégories sont associées à des fréquences relativement similaires, il devient très difficile de repérer leur différence. Dans la `r pie_data3.1_freq`, les deux premières catégories semblent de taille égale, alors qu'elles ne le sont pas. Il est nettement plus aisé de le voir dans la `r barplot_data3.1_freq`. C'est dû au fait que notre oeil repère plus facilement une différence d'hauteur entre deux objets alignés (deux rectangles du diagramme en bâtons) qu'une différence d'aire dans un cercle. Pour cette raison, je vous recommanderai de privilégier autant que possible le diagramme en bâtons. La simplicité est la clé d'un bon graphique!
                                                                          
##### Diagramme des fréquences cumulées

Le diagramme des fréquences cumulées consiste à représenter les différentes valeurs que peut prendre la variable à représenter par des bâtonnets dont la hauteur varie en fonction non plus de la fréquence (absolue ou relative) mais en fonction de la fréquence **cumulée** associée à cette valeur. Le dernier bâton du diagramme des fréquences cumulées aura nécessairement une hauteur égale à n si l'on représente les fréquences absolues, et une auteur égale à 1 si l'on représente les fréquences relatives. Une fois encore, l'allure générale du graphique est la même, que l'on représente les fréquences absolues ou relatives sur le diagramme. En revanche, l'information donnée est un petit peu différente: dans le premier cas, on peut dénombrer le nombre de ménages dont le nombre d'enfants atteint au maximum la valeur représentée par le bâtonnet, dans le second cas, on peut déterminer le pourcentage de ménages dont le nombre d'enfants atteint au maximum la valeur représentée par le bâtonnet.

`r cumdiag_data3.1_freq`  
_Diagramme des fréquences cumulées de la distribution de la `r table3.1_freq`_
```{r barplotcum_data3.1, echo=FALSE}
par(mfrow=c(1,2))
barplot(abscum_data3.1,yaxt = "n",ylim=c(0,length(data3.1_brut$Enfant)),main="Nombre d'enfants par ménages",cex.main=.9,col="lightblue",ylab="Fréquence absolue")
axis(2, at = seq(0,length(data3.1_brut$Enfant),2), labels = seq(0,length(data3.1_brut$Enfant),2), las = 1)

barplot(relcum_data3.1,yaxt = "n",ylim=c(0,100),main="Nombre d'enfants par ménages",cex.main=.9, col="lightblue",ylab="Fréquence relative (en %)")
axis(2, at = seq(0,100,5), labels = seq(0,100,5),las = 2,cex.axis=.8)
```

Il arrive que l'on parle de "diagramme en escalier", et que l'on choisisse plutôt ce mode de représentation, dans lequel un point noir indique la fréquence associée  à une catégorie en particulier. L'information fournie par les diagrammes en escalier et les diagrammes des fréquences cumulées est identique.

`r stair_data3.1_freq`  
_Diagramme en escalier représentant les fréquences cumulées de la distribution de la `r table3.1_freq`_
```{r Enf_nb4, echo=FALSE}
par(mfrow=c(1,2),mar=c(4,2,4,2))
                                                                          plot(names_data3.1,pch=1,abscum_data3.1,type="s",ylim=c(0,length(Enfant)),lty=1,ylab="Fréquence absolue",xlab="")
                                                                          segments(1,0,1,abs_data3.1[1])
for (i in 1:length(names_data3.1)){                                                                    points(i,abscum_data3.1[i],pch=19)
points(i+1,abscum_data3.1[i],pch=19,col="white")
points(i+1,abscum_data3.1[i])
}
                                                                plot(names_data3.1,pch=1,relcum_data3.1,type="s",ylim=c(0,100),lty=1,ylab="Fréquence relative",xlab="")
segments(1,0,1,rel_data3.1[1])
for (i in 1:length(names_data3.1)){
points(i,relcum_data3.1[i],pch=19)
points(i+1,relcum_data3.1[i],pch=19,col="white")
points(i+1,relcum_data3.1[i])
}
```

### Variables quantitatives continues

#### Simplification des données

Dans la mesure où les variables _**continues**_, il  n'est pas possible d'associer une fréquence à chaque valeur possible de la variable, similairement à ce que l'on fait pour des variables discrètes. On procédera alors à des regroupements des valeurs en classes (d'amplitude à déterminer), et on déterminera les fréquences associées à ces classes. 

Il existe de nombreux critères pour déterminer le nombre de classes. De manière générale, plus on aura de classes, plus grande sera la précision. Lorsque l'on doit effectuer les calculs manuellement, on se contentera généralement d'une dizaine de classes. Le choix peut également être guidé par des raisons théoriques(catégories pré-existantes dans la nature). Lorsqu'on utilise des logiciels informatiques prévus à cet effet, on peut par contre travailler avec beaucoup plus de classes (soit des classes ayant une amplitude nettement plus faible), ce qui permettra de se faire une idée bien plus précise de l'allure des données, comme nous y reviendrons à la fin de cette sous-section (voir la partie sur les distributions statistiques).   

Lorsque c'est possible, il est préférable d'utiliser des classes ayant toutes la même amplitude, tel que dans la `r table3.2_freq` et la `r table3.3_freq_a`, notamment pour des raisons de lisibilité graphique. Cependant, il peut arriver que pour des raisons théoriques, nous soyons amenés à envisager des classes de tailles différentes. Par exemple, si l'on veut distinguer des caractéristiques de la petite enfance, enfance, adolescence, âge adulte et troisième âge, on pourra créer 5 classes: une regroupant les individus	de	0	à	3 ans, de ceux de 3 à 12 ans ,	de	 12 à 18 ans, 18 à 65	ans et de plus de 65 ans (voir la `r table3.3_freq_b`).

Enfin, le **centre de classe** est la valeur qui se trouve à égale distance de la borne inférieure et de la borne supérieure d'une classe. Connaître cette valeur servira à estimer les **quantiles** de variables continues (ce concept sera très prochainement expliqué). 

```{r echo=FALSE, results='asis'}
Pays<-c("Luxembourg","Norvâge","Suisse","Irlande","Danemark","Suède","Pays-Bas","Islande","Autriche","Finlande","Allemagne","Belgique","Andorre","France","Royaume-Uni","Zone Euro","Union Européenne","Italie","Espagne","Chypre","Malte","Slovénie","Portugal","Grêce","République Tchèque","Slovaquie","Estonie","Lituanie","Pologne","Hongrie","Lettonie","Croatie","Turquie","Russie","Roumanie","Bulgarie","Monténégro","Biélorussie","Serbie","Bosnie-Herzégovine","Macédoine","Albanie","Kosovo","Ukraine","Moldavie")

PIB_hab<-c(107865.27,91218.62,76667.44,74433.46,61582.17,56935.19,53597.83,49910.01,49129.23,47057.62,46747.19,46078.93,43942.9,42567.74,42514.49,40088.65,36593.03,34877.83,32405.75,29432.67,27145.81,25662.41,23116.58,23027.41,22779.29,19897.15,18977.39,16793.25,15751.23,15647.85,15553.33,15219.88,14933.27,11441,10932.33,8311.93,7812.95,6375.82,5992.28,5561.29,5245.36,4868.2,4068.21,2991.63,2165.16)

mini=0
maxi=120000
nb_categ_PIB=6
categ_PIB=NULL
centre_PIB=NULL

for(j in 1:nb_categ_PIB){
  categ_PIB[j]=paste0("[",as.numeric((j-1)*(maxi-mini)/nb_categ_PIB),"-",as.numeric(j*(maxi-mini)/nb_categ_PIB),"[")  
  centre_PIB[j]=(as.numeric((j-1)*(maxi-mini)/nb_categ_PIB)+as.numeric(j*(maxi-mini)/nb_categ_PIB))/2
}

data3.2_brut=data.frame(Pays,PIB_hab)
```

```{r echo=FALSE, results='asis'}
abs_data3.2=NULL

for(i in 1:nb_categ_PIB){
  abs_data3.2[i]=sum((data3.2_brut$PIB_hab>=((i-1)*((maxi-mini)/nb_categ_PIB)))&(data3.2_brut$PIB_hab< (i*(maxi-mini)/nb_categ_PIB)))
}
rel_data3.2=abs_data3.2/sum(abs_data3.2)*100

abscum_data3.2=NULL
relcum_data3.2=NULL
count_abs_data3.2=0
count_rel_data3.2=0
for (i in 1:length(categ_PIB)){
  count_abs_data3.2=count_abs_data3.2+abs_data3.2[i]
  abscum_data3.2[i]=count_abs_data3.2
  count_rel_data3.2=count_rel_data3.2+rel_data3.2[i]
  relcum_data3.2[i]=count_rel_data3.2
}

data3.2_freq=data.frame(categ_PIB,centre_PIB,abs_data3.2,round(count_rel_data3.2,2),abscum_data3.2,round(relcum_data3.2,2)) 
```

`r table3.2_freq`  
_Table des PIB par habitants de `r length(PIB_hab)` pays d'Europe_
```{r echo=FALSE, results='asis'}
kable(data3.2_freq,format="markdown",align="c",col.names=c("Classe","Centre de classe","$n_j$","$f_j$ (en%)","$N_j$","$F_j$ (en %)") ) 
```
Source: https://fr.tradingeconomics.com/country-list/gdp-per-capitaécontinent=europe

```{r echo=FALSE, results='asis'}
Age_discret<-rep(c(1,1,2,2,1,1,2,1,2,1,2,2,1,6,8,8,3,11,8,6,10,5,8,3,10,3,7,6,6,6,6,7,14,16,17,17,14,12,13,12,12,17,15,17,16,14,16,15,15,12,12,14,14,17,46,44,48,34,55,40,22,53,24,40,54,45,34,41,30,25,49,34,22,23,61,32,58,63,21,22,33,58,61,31,19,44,24,30,50,21,29,38,62,23,39,41,64,50,45,39,54,27,55,32,63,31,18,23,63,57,50,28,60,58,29,61,48,42,19,28,26,40,26,60,49,30,45,22,19,64,24,52,61,34,37,21,54,53,24,24,25,22,26,41,35,35,27,23,31,21,19,25,19,57,38,23,61,39,59,53,33,58,31,61,23,64,50,28,58,77,99,84,82,85,97,72,84,84,81,82,76,103,102,96,77,67,93,69,93,82,79,77,71,69,81,96,90,65,95,87),100)
set.seed(1)
Bruit=rnorm(length(Age_discret),0,.3)
Age=Age_discret+Bruit
id=1:length(Age)

mini=0
maxi=110
nb_categ_Age_a=11
categ_Age_a=NULL
centre_Age_a=NULL

for(j in 1:nb_categ_Age_a){
categ_Age_a[j]=paste0("[",as.numeric((j-1)*(maxi-mini)/nb_categ_Age_a),"-",as.numeric(j*(maxi-mini)/nb_categ_Age_a),"[")  
  centre_Age_a[j]=(as.numeric((j-1)*(maxi-mini)/nb_categ_Age_a)+as.numeric(j*(maxi-mini)/nb_categ_Age_a))/2
}

data3.3_brut=data.frame(id,Age)
```

```{r echo=FALSE, results='asis'}
abs_data3.3.1=NULL
for(i in 1:nb_categ_Age_a){
abs_data3.3.1[i]=sum((data3.3_brut$Age>=((i-1)*((maxi-mini)/nb_categ_Age_a))) &(data3.3_brut$Age< (i*(maxi-mini)/nb_categ_Age_a)))
}
rel_data3.3.1=abs_data3.3.1/sum(abs_data3.3.1)*100

abscum_data3.3.1=NULL
relcum_data3.3.1=NULL
count_abs_data3.3.1=0
count_rel_data3.3.1=0

for (i in 1:length(categ_Age_a)){
count_abs_data3.3.1=count_abs_data3.3.1+abs_data3.3.1[i]
abscum_data3.3.1[i]=count_abs_data3.3.1
count_rel_data3.3.1=count_rel_data3.3.1+rel_data3.3.1[i]
relcum_data3.3.1[i]=count_rel_data3.3.1
}

data3.3_freq_a=data.frame(categ_Age_a,centre_Age_a,abs_data3.3.1,round(rel_data3.3.1,2),abscum_data3.3.1,round(relcum_data3.3.1,2))
```


```{r echo=FALSE, results='asis'}
min_categ_b=c(0,3,12,18,65)
max_categ_b=c(3,12,18,65,110)
categ_Age_b=NULL
centre_Age_b=NULL

for (i in 1:length(min_categ_b)){
categ_Age_b[i]=paste0("[",min_categ_b[i],"-",max_categ_b[i],"[")
centre_Age_b[i]=(min_categ_b[i]+max_categ_b[i])/2
}

abs_data3.3.2=NULL
for(i in 1:length(min_categ_b)){
abs_data3.3.2[i]=sum((data3.3_brut$Age>=min_categ_b[i])&(data3.3_brut$Age<max_categ_b[i]))
}
rel_data3.3.2=abs_data3.3.2/sum(abs_data3.3.2)*100

abscum_data3.3.2=NULL
relcum_data3.3.2=NULL
count_abs_data3.3.2=0
count_rel_data3.3.2=0
for (i in 1:length(categ_Age_b)){
count_abs_data3.3.2=count_abs_data3.3.2+abs_data3.3.2[i]
abscum_data3.3.2[i]=count_abs_data3.3.2
count_rel_data3.3.2=count_rel_data3.3.2+rel_data3.3.2[i]
relcum_data3.3.2[i]=count_rel_data3.3.2
}

data3.3_freq_b=data.frame(categ_Age_b,centre_Age_b,abs_data3.3.2,round(rel_data3.3.2,2),abscum_data3.3.2,round(relcum_data3.3.2,2)) 
```

`r table3.3_freq_a`  
_Table de fréquence de catégories d'âges dans un échantillon de `r length(Age)` personnes_
```{r echo=FALSE, results='asis'}
kable(data3.3_freq_a,format="markdown",align="c",col.names=c("Classe","Centre","$n_j$","$f_j$ (en%)","$N_j$","$F_j$ (en %)")) 

```

`r table3.3_freq_b`  
_Table de fréquence de catégories d'âges dans un échantillon de `r length(Age)` personnes_
```{r echo=FALSE, results='asis'}
kable(data3.3_freq_b,format="markdown",align="c",col.names=c("Classe","Centre","$n_j$","$f_j$ (en%)","$N_j$","$F_j$ (en %)")) 
```

#### Réalisation graphique

##### Histogramme

Le graphique le plus couramment utilisé pour représenter des variables continues est l'histogramme. Un histogramme ressemble à un diagramme en bâtons, mais au lieu d'avoir des bâtons isolés qui représentent une valeur unique, on a des rectangles, collés les uns aux autres (pour rendre compte du caractère continu de la variable) qui représentent une classe de valeurs. 

Les classes de valeurs sont représentées en abcisse. Lorsque toutes les classes sont de taille égale, la hauteur des rectangles est proportionnelle aux fréquences (absolues ou relatives; les fréquences correspondent à l'ordonnée), tel qu'on peut le voir dans la `r barplot_data3.1_freq` ainsi que dans la `r barplot_data3.3_freq_a`   .  

`r barplot_data3.2_freq`  
_Histogramme de la distribution de la `r table3.2_freq`_
```{r PIB, echo=FALSE}

par(mfrow=c(1,2),mar=c(4,2,2,2))

hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence absolue",cex.lab=1,xlab="PIB",las=2,cex.axis=.8)

hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence relative",cex.lab=1,xlab="PIB",yaxt='n',las=2,cex.axis=.8)
axis(2, at = seq(0,max(abs_data3.2),2), labels = round(seq(0,max(abs_data3.2),2)/sum(abs_data3.2),2),las = 2,cex.axis=.8)
```

`r barplot_data3.3_freq_a`  
_Histogramme de la distribution de la `r table3.3_freq_a`_ 
```{r Agea, echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,2,2))

hist(data3.3_brut$Age,breaks=length(categ_Age_a),xlim=c(0,110),col="lightgrey",cex.main=.8,main="Histogramme de l'âge",ylab="Fréquence absolue",cex.lab=1,xlab="Age",las=2,cex.axis=.8,xaxt="n")
axis(1, at = seq(0,110,10), labels = seq(0,110,10),las = 2,cex.axis=.8)

hist(data3.3_brut$Age,breaks=length(categ_Age_a),xlim=c(0,110),col="lightgrey",cex.main=.8,main="Histogramme de l'âge",ylab="Fréquence relative",cex.lab=1,xlab="Age",yaxt='n',xaxt="n",las=2,cex.axis=.8)
limsup_y=max(abs_data3.3.1)+100-max(abs_data3.3.1)%%100
axis(1, at = seq(0,110,10), labels = seq(0,110,10),las = 2,cex.axis=.8)
axis(2, at = seq(0,limsup_y,limsup_y/7), labels = round(seq(0,limsup_y,limsup_y/7)/sum(abs_data3.3.1),2),las = 2,cex.axis=.8)
```

Par contre, lorsque les classes sont de tailles inégales, la hauteur du rectancle est proportionnelle à la densité de la classe (soit l'effectif divisé par l'amplitude de la classe), ce qui rend la comparaison des fréquences des classes plus compliquée.

$h_i=\frac{n_i}{a_i}$

`r barplot_data3.3_freq_b`
_Histogramme de la distribution de la `r table3.3_freq_b`_
```{r ageb, echo=FALSE}
par(mfrow=c(1,1),mar=c(2,2,2,2))

hist(data3.3_brut$Age,breakpoints <- c(0, 3,12,18,65,110),xaxt="n",col="lightgrey",main="Histogramme de l'âge",ylab="Densité",cex.lab=1,xlab="Age")
axis(1, at = c(0, 3,12,18,65,110), labels = c(0, 3,12,18,65,110),las = 2,cex.axis=.8)
```

Dans la mesure où les données sont continues, cela signifie que toutes les valeurs de la variable représentée sont considérées comme possibles. Par exemple, le premier rectangle de la `r barplot_data3.3_freq_a` englobe toutes les personnes dont l'âge est compris entre 0 et 10 ans (incluant les personnes ayant 2,38574 ans), et lorsque je me déplace du premier centre de classe vers le deuxième centre de classe, j'envisage n'importe quelle valeur comprise entre `r centre_Age_a[1]` et `r centre_Age_a[2]`. Pour mieux rendre compte de la continuité de la variable, on peut tracer ce qu'on appelle le **polygone des effectifs**, en reliant par des segments de droite le centre du côté supérieur de chaque rectangle.  
  
`r polygone_data3.2_freq`  
_Histogramme de la distribution de la `r table3.2_freq` et polygone des effectifs_
```{r polygon_PIB, echo=FALSE}
par(mfrow=c(1,2),mar=c(4,2,2,2),xpd=TRUE)
amplitude=as.numeric((2)*(maxi-mini)/nb_categ_PIB)-as.numeric((1)*(maxi-mini)/nb_categ_PIB)

hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence absolue",cex.lab=1,xlab="PIB",las=2,cex.axis=.8,xlim=c(0,120000))
segments(centre_PIB[1]-(amplitude),0,centre_PIB[1],abs_data3.2[1],lwd=2)  
segments(centre_PIB[length(centre_PIB)],abs_data3.2[length(centre_PIB)],centre_PIB[length(centre_PIB)]+(amplitude),0,lwd=2)  
for(i in 1:(length(centre_PIB))){
segments(centre_PIB[i],abs_data3.2[i],centre_PIB[i+1],abs_data3.2[i+1],lwd=2)
points(centre_PIB[i],abs_data3.2[i],pch=16)
}

hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence relative",cex.lab=1,xlab="PIB",yaxt='n',las=2,cex.axis=.8,xlim=c(0,120000))
axis(2, at = seq(0,max(abs_data3.2),2), labels = round(seq(0,max(abs_data3.2),2)/sum(abs_data3.2),2),las = 2,cex.axis=.8)
segments(centre_PIB[1]-(amplitude),0,centre_PIB[1],abs_data3.2[1],lwd=2)  
segments(centre_PIB[length(centre_PIB)],abs_data3.2[length(centre_PIB)],centre_PIB[length(centre_PIB)]+(amplitude),0,lwd=2)  
for(i in 1:(length(centre_PIB))){
segments(centre_PIB[i],abs_data3.2[i],centre_PIB[i+1],abs_data3.2[i+1],lwd=2)  
points(centre_PIB[i],abs_data3.2[i],pch=16)
}

```

`r polygone_data3.3_freq_a`  
_Histogramme de la distribution de la `r table3.3_freq_a` et polygone des effectifs_ 
```{r polygon_Agea, echo=FALSE}
par(mfrow=c(1,2),mar=c(2,4,2,2),xpd=TRUE)

amplitude=as.numeric((2)*(maxi-mini)/nb_categ_Age_a)-as.numeric((1)*(maxi-mini)/nb_categ_Age_a)

hist(data3.3_brut$Age,breaks=length(categ_Age_a),xlim=c(0,110),col="lightgrey",cex.main=.8,main="Histogramme de l'âge",ylab="Fréquence absolue",cex.lab=1,xlab="Age",las=2,cex.axis=.8,xaxt="n")
axis(1, at = seq(0,110,10), labels = seq(0,110,10),las = 2,cex.axis=.8)
segments(centre_Age_a[1]-(amplitude),0,centre_Age_a[1],abs_data3.3.1[1],lwd=2)  
segments(centre_Age_a[length(centre_Age_a)],abs_data3.3.1[length(centre_Age_a)],centre_Age_a[length(centre_Age_a)]+(amplitude),0,lwd=2)  
for(i in 1:(length(centre_Age_a))){
segments(centre_Age_a[i],abs_data3.3.1[i],centre_Age_a[i+1],abs_data3.3.1[i+1],lwd=2)  
points(centre_Age_a[i],abs_data3.3.1[i],pch=16)
}


hist(data3.3_brut$Age,breaks=length(categ_Age_a),xlim=c(0,110),col="lightgrey",cex.main=.8,main="Histogramme de l'âge",ylab="Fréquence relative",cex.lab=1,xlab="Age",yaxt='n',xaxt="n",las=2,cex.axis=.8)
limsup_y=max(abs_data3.3.1)+100-max(abs_data3.3.1)%%100
axis(1, at = seq(0,110,10), labels = seq(0,110,10),las = 2,cex.axis=.8)
axis(2, at = seq(0,limsup_y,limsup_y/7), labels = round(seq(0,limsup_y,limsup_y/7)/sum(abs_data3.3.1),2),las = 2,cex.axis=.8)
segments(centre_Age_a[1]-(amplitude),0,centre_Age_a[1],abs_data3.3.1[1],lwd=2)  
segments(centre_Age_a[length(centre_Age_a)],abs_data3.3.1[length(centre_Age_a)],centre_Age_a[length(centre_Age_a)]+(amplitude),0,lwd=2)  
for(i in 1:(length(centre_Age_a))){
segments(centre_Age_a[i],abs_data3.3.1[i],centre_Age_a[i+1],abs_data3.3.1[i+1],lwd=2)  
points(centre_Age_a[i],abs_data3.3.1[i],pch=16)
}
```

##### Histogramme des effectifs (ou fréquences) cumulé(e)s

Les histogrammes des effectifs (ou fréquences) cumulées ressemblent aux diagrammes des fréquences cumulées, si ce n'est que cette fois, les différents bâtons sont collés les uns aux autres pour rendre compte du caractère continu de la variable. 

De même que pour les histogrammes vu dans la section précédente, il est possible de tracer le polygone des effectifs (ou fréquences) cumulé(e)s pour mieux rendre compte du caractère continu de la variable. 

`r polygone_data3.2_freq`  
_Histogramme de la distribution de la `r table3.2_freq` et polygone des effectifs_
```{r cumpolygon_PIB, echo=FALSE}

amplitude=centre_PIB[2]-centre_PIB[1]
par(mfrow=c(1,2),xpd=TRUE)
h=hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,plot=FALSE)
h$counts <- cumsum(h$counts) # replace the cell freq.s by cumulative freq.s
plot(h,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence absolue",cex.lab=1,xlab="PIB",las=2,cex.axis=.8,xlim=c(0,120000),ylim=c(0,length(data3.2_brut$PIB_hab)),yaxt="n") # plot a cumulative histogram of y
axis(2, at = seq(0,length(data3.2_brut$PIB_hab),5), labels = seq(0,length(data3.2_brut$PIB_hab),5),las = 2,cex.axis=.8)
segments(centre_PIB[1]-(amplitude),0,centre_PIB[1],abs_data3.2[1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
for(i in 1:(length(centre_PIB))){
segments(centre_PIB[i],abscum_data3.2[i],centre_PIB[i+1],abscum_data3.2[i+1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
}


h=hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,plot=FALSE)
h$counts <- cumsum(h$counts) # replace the cell freq.s by cumulative freq.s
plot(h,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence relative",cex.lab=1,xlab="PIB",las=2,cex.axis=.8,xlim=c(0,120000),ylim=c(0,length(data3.2_brut$PIB_hab)),yaxt="n") # plot a cumulative histogram of y
axis(2, at = seq(0,length(data3.2_brut$PIB_hab),5), labels = round(seq(0,length(data3.2_brut$PIB_hab),5)/length(data3.2_brut$PIB_hab),2),las = 2,cex.axis=.8)
segments(centre_PIB[1]-(amplitude),0,centre_PIB[1],abs_data3.2[1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
for(i in 1:(length(centre_PIB))){
segments(centre_PIB[i],abscum_data3.2[i],centre_PIB[i+1],abscum_data3.2[i+1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
}
```

#### Remarque: les distributions 

Il a été brièvement mentionné que lorsque le nombre de classe augmente (ce qui revient à dire que l'amplitude de chaque classe diminue), l'allure générale des données se dessine avec plus de précision. Cela est illustré dans la `r Figuredistribution`, où les mêmes données sont représentées à l'aide de trois histogrammes, dont le nombre de classes varie (`r nb_categ_Age_a` classes dans l'histogramme de gauche,`r nb_categ_Age_a*5` classes dans l'histogramme du milieu et `r nb_categ_Age_a*10000` classes dans l'histogramme de droite). Au fur et à mesure que le nombre de classes augmentent, l'allure de l'histogramme se rapproche d'une courbe, jusqu'à se lisser parfaitement. Cette courbe lisse représente ce qu'on appelle la distribution des données.   

`r Figuredistribution`  
_De l'histogramme vers la distribution de données_
```{r Figuredistribution, echo=FALSE}
par(mfrow=c(1,3),mar=c(3,.5,1,.5))
n=10000000
Newexemple=rnorm(n,0,5)
hist(Newexemple,breaks=nb_categ_Age_a,main=paste(nb_categ_Age_a,"classes"),yaxt="n",xlab="",xlim=c(-30,30))
hist(Newexemple,breaks=nb_categ_Age_a*5,main=paste(nb_categ_Age_a*5, "classes"),yaxt="n",xlab="",xlim=c(-30,30))
hist(Newexemple,breaks=nb_categ_Age_a*10000,main=paste(nb_categ_Age_a*10000,"classes"),yaxt="n",xlab="",xlim=c(-30,30))
```

## Les graphiques construits sur base des quantiles

Outre les tableaux de fréquence, une autre manière utile de décrire une série de données ou une distribution est de la diviser en un certain nombre d'intervalles contenant tous le même nombre d'observations. Les bornes de  ces intervalles sont appelés des **quantiles**. Contrairement aux tableaux de fréquence, le calcul des quantiles n'a de sens que pour décrire des variables **quantitatives** (discrètes ou continues).

Certains quantiles sont particulièrement connus, on retrouve parmi eux:

- La **Médiane** qui découpe la distribution en deux parties contenant chacune 50% des observations. Ce quantile, très important, donne une mesure de ce qu'on appelle la **tendance centrale** (cette notion sera développée au chapitre suivant). 
- Les **Quartiles** qui découpent la distribution en 4 parties contenant chacune 25% des observations. 
- Les **Déciles** qui découpent la distribution en 10 parties contenant chacune 10% des observations
- Les **Percentiles** qui découpent la distribution en 100 parties contenant chacune 1% des observations. 

Remarquez que le 25ème percentile est le premier quartile, le cinquantième percentile est le cinquième décile ou le deuxième quartile ou encore la médiane, etc.

Bien que le principe soit identique quel que soit le quantile envisagé, nous ne développerons que la **médiane** et les **quartiles**, parce qu'ils sont nécessaires à la réalisation de boîtes à moustaches que nous élaborerons ensuite. Par ailleurs, on ne calculera pas ces quantiles exactement de la même manière suivant que les données soit discrètes ou continues. 

### Variables quantitatives discrètes

#### Simplification des données: calcul de la médiane et des quartiles

##### Au départ de données brutes

```{r med, echo=FALSE,include=FALSE}
serie1=c(3,7,2,11,9,8,1,13,15)
serie1_ordered=sort(serie1)
serie2=c(3,7,2,6,9,8,11,14,13,15)
serie2_ordered=sort(serie2)
serie3=c(3,8,2,6,7,8,11,14,13,15)
serie3_ordered=sort(serie3)
```

Soit une série statistique contenant un nombre impair de sujets (n=`r length(serie1)`): $$ **Série 1**   `r serie1`$$

La médiane va consister à déterminer une valeur telle qu'il y ait la moitié des observations de la série dont la valeur lui est inférieure, et l'autre moitié dont la valeur lui est supérieure. Dans un premier temps, il faudra ordonner la série par ordre croissant. Ensuite, on pourra déterminer la position à laquelle se trouve la médiane. Il s'agira de la valeur qui occupe le rang $\frac{n+1}{2}$. 

Voici la série ordonnée: $$`r serie1_ordered`$$.   

La médiane sera l'observation qui occupera le rang $\frac{`r (length(serie1))`+1}{2}$ (soit la $`r (length(serie1)+1)/2`^e$ observation) de la série ordonnée. Or, la $`r (length(serie1)+1)/2`^e$ observation de la série ordonnée a pour valeur `r serie1_ordered[(length(serie1)+1)/2]`. La médiane vaut donc `r serie1_ordered[(length(serie1)+1)/2]`. 

Attention: il ne faut pas confondre le **rang médian** (soit la position occupée par la médiane dans la série) et la **médiane** (la valeur de l'observation située au rang médian). 

Soit une autre série statistique, contenant cette fois un nombre pair de sujets (n=`r length(serie2)`):$$ **Série 2**   `r serie2`$$
On commence par ordonnée la série: $$`r serie2_ordered`$$
On calcule ensuite le rang médian: $\frac{`r length(serie2)`+1}{2}=`r (length(serie2)+1)/2`$. Cette fois, le rang n'est pas un nombre rond. On en déduit que la médiane sera la valeur entre la $`r floor((length(serie2)+1)/2)`^e$ observation (valant `r  serie2_ordered[floor((length(serie2)+1)/2)]`) et la $`r ceiling((length(serie2)+1)/2)`^e$ observation de la série (valant `r serie2_ordered[ceiling((length(serie2)+1)/2)]`). Par convention, on prend la moyenne de ces deux valeurs, ce qui donne `r (serie2_ordered[floor((length(serie2)+1)/2)]+serie2_ordered[ceiling((length(serie2)+1)/2)])/2`.   

Comme ces deux exemples permettent de l'illustrer, avec un nombre impair d'observations, la valeur de la médiane correspond nécessairement à une valeur observée de la variable. Par contre, avec un nombre pair d'observations, la valeur de la médiane ne correspond "pas nécessairement" à une valeur observée de la médiane. Attention, pas nécessairement ne veut pas dire jamais: imaginons que les deux valeurs de part et d'autre du rang moyen soient identiques, comme dans la série ordonnée suivante:   
$$**Série 3** `r serie3_ordered`$$.  
Dans ce cas, étant donné que le rang médian vaut `r (length(serie3)+1)/2`, la médiane sera la moyenne entre `r serie3_ordered[floor((length(serie3)+1)/2)]` et `r serie3_ordered[ceiling((length(serie3)+1)/2)]`, soit `r (serie3_ordered[floor((length(serie3)+1)/2)]+serie3_ordered[ceiling((length(serie3)+1)/2)])/2`. 

La valeur du premier quartile d'une série sera celle qui est telle que si l'on s'intéresse exclusivemenet aux valeurs à gauche de la médiane, la moitié des observations sélectionnées lui est inférieure et l'autre moitié lui est supérieure. Similairement, la valeur du troisième quartile sera celle qui est telle que si l'on s'intéresse exclusivement aux valeurs à droite de la médiane, la moitié des observations sélectionnées lui est inférieure et l'autre moitié lui est supérieure.

```{r med_value, echo=FALSE,include=FALSE}
quartiles=function(serie){
ordered_serie=sort(serie)
med_rank=(length(serie)+1)/2
if (med_rank%%1==0){
  med_value=ordered_serie[med_rank]
  Q1_serie=ordered_serie[1:((length(serie)+1)/2)-1]
  Q3_serie=ordered_serie[(((length(serie)+1)/2)+1):(length(serie))]
  } else {
  A=ordered_serie[med_rank-.5]
  B=ordered_serie[med_rank+.5]
  med_value=(A+B)/2
  Q1_serie=ordered_serie[1:(med_rank-.5)]
  Q3_serie=ordered_serie[(med_rank+.5):length(serie)]
  }

Q1_rank=(length(Q1_serie)+1)/2
  if(Q1_rank%%1==0){
  Q1=Q1_serie[Q1_rank]  
  } else{
  C=Q1_serie[Q1_rank-.5]
  D=Q1_serie[Q1_rank+.5]
  Q1=(C+D)/2
  }

Q3_rank=(length(Q3_serie)+1)/2
  if(Q3_rank%%1==0){
  Q3=Q3_serie[Q3_rank]  
  } else{
  F=Q3_serie[Q3_rank-.5]
  G=Q3_serie[Q3_rank+.5]
  Q3=(F+G)/2
  }
return(list(quartiles=c(Q1=Q1,med=med_value,Q3=Q3),Q1_serie=Q1_serie,Q3_serie=Q3_serie))
}
```

Par exemple, si l'on revient à la série 1, soit la série suivante:  $`r serie1_ordered`$

* La partie des données à gauche de la médiane est la suivante: `r quartiles(serie1)[[2]]`

La valeur du premier quartile sera donc égal à `r quartiles(serie1)[[1]][1]`.

* La partie à droite de la médiane est la partie suivante: `r quartiles(serie1)[[3]]`  

La valeur du troisième quartile sera donc égal à `r quartiles(serie1)[[1]][3]`.

Si l'on revient à la série 2, soit la série suivante:  $`r serie2_ordered`$

* La partie des données à gauche de la médiane est la suivante: `r quartiles(serie2)[[2]]`

La valeur du premier quartile sera donc égal à `r quartiles(serie2)[[1]][1]`.

* La partie à droite de la médiane est la partie suivante: `r quartiles(serie2)[[3]]`  

La valeur du troisième quartile sera donc égal à `r quartiles(serie2)[[1]][3]`.

Enfin, si l'on revient à la série 3, soit la série suivante:  $`r serie3_ordered`$

* La partie des données à gauche de la médiane est la suivante: `r quartiles(serie3)[[2]]`

La valeur du premier quartile sera donc égal à `r quartiles(serie3)[[1]][1]`.

* La partie à droite de la médiane est la partie suivante: `r quartiles(serie3)[[3]]`  

La valeur du troisième quartile sera donc égal à `r quartiles(serie3)[[1]][3]`.

##### Au départ de tableaux de fréquences

La `r table3.6_serie1` est le tableau de fréquence des données de la série 1.

```{r,include=TRUE, echo=FALSE, results='asis'}
names_serie1=names(table(serie1))
abs_serie1=tabulate(serie1)[tabulate(serie1)!=0]
rel_serie1=round(abs_serie1/length(serie1),2)

abscum_serie1=NULL
count_abs=0

for (i in 1:length(names_serie1)){
  count_abs=count_abs+abs_serie1[i]
  abscum_serie1[i]=count_abs
}

relcum_serie1=round(abscum_serie1/length(serie1),2)

serie1_freq=data.frame(names_serie1,abs_serie1,rel_serie1,abscum_serie1,relcum_serie1)
kable(serie1_freq,format="markdown",align="c",col.names = c("$x_{j}$","$n_{j}$","$f_{j}$","$N_{j}$","$F_{j}$"))
```

La fréquence cumulée associée à la valeur `r max(as.numeric(names_serie1[relcum_serie1<=.5]))` vaut `r relcum_serie1[names_serie1==max(as.numeric(names_serie1[relcum_serie1<=.5]))]`. Dans la mesure où la médiane est la valeur telle qu'il y ait 50% des observations à gauche de celle-ci, elle sera nécessairement supérieure à `r max(as.numeric(names_serie1[relcum_serie1<=.5]))`.  La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie1[relcum_serie1>=.5]))` vaut `r relcum_serie1[names_serie1==min(as.numeric(names_serie1[relcum_serie1>=.5]))]`. La médiane vaudra donc `r min(as.numeric(names_serie1[relcum_serie1>=.5]))`.  

En suivant un raisonnement identique:   

- la fréquence cumulée associée à la valeur `r 
max(as.numeric(names_serie1[relcum_serie1<=.25]))` vaut `r relcum_serie1[names_serie1==max(as.numeric(names_serie1[relcum_serie1<=.25]))]`. La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie1[relcum_serie1>=.25]))` vaut `r relcum_serie1[names_serie1==min(as.numeric(names_serie1[relcum_serie1>=.25]))]`. La médiane vaudra donc `r min(as.numeric(names_serie1[relcum_serie1>=.25]))`.  

- la fréquence cumulée associée à la valeur `r 
max(as.numeric(names_serie1[relcum_serie1<=.75]))` vaut `r relcum_serie1[names_serie1==max(as.numeric(names_serie1[relcum_serie1<=.75]))]`. La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie1[relcum_serie1>=.75]))` vaut `r relcum_serie1[names_serie1==min(as.numeric(names_serie1[relcum_serie1>=.75]))]`. La médiane vaudra donc `r min(as.numeric(names_serie1[relcum_serie1>=.75]))`.  

La `r table3.6_serie3` est le tableau de fréquence des données de la série 3.

```{r,include=TRUE, echo=FALSE, results='asis'}
names_serie3=names(table(serie3))
abs_serie3=tabulate(serie3)[tabulate(serie3)!=0]
rel_serie3=round(abs_serie3/length(serie3),2)

abscum_serie3=NULL
count_abs=0

for (i in 1:length(names_serie3)){
  count_abs=count_abs+abs_serie3[i]
  abscum_serie3[i]=count_abs
}

relcum_serie3=round(abscum_serie3/length(serie3),2)

serie3_freq=data.frame(names_serie3,abs_serie3,rel_serie3,abscum_serie3,relcum_serie3)
kable(serie3_freq,format="markdown",align="c",col.names = c("$x_{j}$","$n_{j}$","$f_{j}$","$N_{j}$","$F_{j}$"))
```

La fréquence cumulée associée à la valeur `r max(as.numeric(names_serie3[relcum_serie3<=.5]))` vaut `r relcum_serie3[names_serie3==max(as.numeric(names_serie3[relcum_serie3<=.5]))]`. Dans la mesure où la médiane est la valeur telle qu'il y ait 50% des observations à gauche de celle-ci, elle sera nécessairement supérieure à `r max(as.numeric(names_serie3[relcum_serie3<=.5]))`.  La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie3[relcum_serie3>=.5]))` vaut `r relcum_serie3[names_serie3==min(as.numeric(names_serie3[relcum_serie3>=.5]))]`. La médiane vaudra donc `r min(as.numeric(names_serie3[relcum_serie3>=.5]))`.  

En suivant un raisonnement identique:   

- la fréquence cumulée associée à la valeur `r 
max(as.numeric(names_serie3[relcum_serie3<=.25]))` vaut `r relcum_serie3[names_serie3==max(as.numeric(names_serie3[relcum_serie3<=.25]))]`. La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie3[relcum_serie3>=.25]))` vaut `r relcum_serie3[names_serie3==min(as.numeric(names_serie3[relcum_serie3>=.25]))]`. La médiane vaudra donc `r min(as.numeric(names_serie3[relcum_serie3>=.25]))`.  

- la fréquence cumulée associée à la valeur `r 
max(as.numeric(names_serie3[relcum_serie3<=.75]))` vaut `r relcum_serie3[names_serie3==max(as.numeric(names_serie3[relcum_serie3<=.75]))]`. La fréquence cumulée associée à la valeur `r min(as.numeric(names_serie3[relcum_serie3>=.75]))` vaut `r relcum_serie3[names_serie3==min(as.numeric(names_serie3[relcum_serie3>=.75]))]`. La médiane vaudra donc `r min(as.numeric(names_serie3[relcum_serie3>=.75]))`.

Enfin, la `r table3.6_serie2` est le tableau de fréquence des données de la série 2.

```{r,include=TRUE, echo=FALSE, results='asis'}
names_serie2=names(table(serie2))
abs_serie2=tabulate(serie2)[tabulate(serie2)!=0]
rel_serie2=round(abs_serie2/length(serie2),2)

abscum_serie2=NULL
count_abs=0

for (i in 1:length(names_serie2)){
  count_abs=count_abs+abs_serie2[i]
  abscum_serie2[i]=count_abs
}

relcum_serie2=round(abscum_serie2/length(serie2),2)

serie2_freq=data.frame(names_serie2,abs_serie2,rel_serie2,abscum_serie2,relcum_serie2)
kable(serie2_freq,format="markdown",align="c",col.names = c("$x_{j}$","$n_{j}$","$f_{j}$","$N_{j}$","$F_{j}$"))
```

Ici, on constate que la fréquence cumulée associée à la valeur `r max(as.numeric(names_serie2[relcum_serie2<=.5]))` vaut exactement `r relcum_serie2[names_serie2==max(as.numeric(names_serie2[relcum_serie2<=.5]))]`. Dans ce cas particulier, par convention, on estimera que la médiane est la moyenne entre cette valeur associée à une fréquence cumulée d'exactement .5 et la valeur suivante de la série. Dans le cas présent, la médiane sera donc la moyenne entre `r max(as.numeric(names_serie2[relcum_serie2<=.5]))` et `r min(as.numeric(names_serie2[relcum_serie2>.5]))`, soit `r (max(as.numeric(names_serie2[relcum_serie2<=.5]))+min(as.numeric(names_serie2[relcum_serie2>.5])))/2`. Vérifiez la valeur de la médiane que nous avions trouvé en analysant les données brutes de la série 3, vous verrez bien qu'il s'agit bien de la valeur que nous avions trouvé.

La valeur des premier et troisième quartiles correspondent aux premières lignes de fréquences cumulées qui dépassent respectivement .25 et .75, soit `r quartiles(serie2)[[1]][1]` et `r quartiles(serie2)[[1]][3]`. 

#### Réalisation graphique: le boxplot (appelée également "boîte à moustaches")

`r table3.7`  
_Tableau de fréquence d'une série de données discrètes_
```{r,include=TRUE, echo=FALSE, results='asis'}
data3.7_brut=c(3,4,4,1,3,1,7,3,8,10,8,4,5,13,2,10,7,9,6,10,6,5,2,6,9,9,9,10,3,4,3,2,2,7,2,2,7,3,7,3,9,6,9,5,9,7,1,7,3,7,3,4,2,1,18)

names_data3.7=names(table(data3.7_brut))
abs_data3.7=tabulate(data3.7_brut)[tabulate(data3.7_brut)!=0]
rel_data3.7=round(abs_data3.7/length(data3.7_brut),2)

abscum_data3.7=NULL
count_abs=0

for (i in 1:length(names_data3.7)){
  count_abs=count_abs+abs_data3.7[i]
  abscum_data3.7[i]=count_abs
}

relcum_data3.7=round(abscum_data3.7/length(data3.7_brut),2)

data3.7_freq=data.frame(names_data3.7,abs_data3.7,rel_data3.7,abscum_data3.7,relcum_data3.7)
kable(data3.7_freq,format="markdown",align="c",col.names = c("$x_{j}$","$n_{j}$","$f_{j}$","$N_{j}$","$F_{j}$"))
```


La `r boxplot_serie1` est une représentation graphique des données de la `r table3.7`. 

`r boxplot_serie1`  
_Boîte à moustache représentant les données de la `r table3.7`_
```{r boxplot_data3.7, echo=FALSE}
boxplot(data3.7_brut,ylim=c(0,20),col="lightblue")
```

L'axe vertical représente les différentes valeurs que peut prendre la variable étudiée. Etant donné que la valeur la plus basse de la série est `r min(data3.7_brut)`, et que sa valeur la plus haute est `r max(data3.7_brut)`, cela signifie que l'ensemble des observations de la série devra être représenté entre ces deux valeurs.

La **boîte centrale** (en bleu) s'étend du premier quartile (la limite inférieure de la boîte centrale = `r quartiles(data3.7_brut)[[1]][1]`) au troisème quartile (= la limite supérieure de la boîte centrale = `r quartiles(data3.7_brut)[[1]][3]`). Elle correspond donc aux 50% des données centrales de la distribution. La distance qui sépare Q1 et Q3 s'appelle **l'écart inter-quartile** et se calcule comme suit: Q3 - Q1 = `r quartiles(data3.7_brut)[[1]][3]`-`r quartiles(data3.7_brut)[[1]][1]`=`r quartiles(data3.7_brut)[[1]][3]-quartiles(data3.7_brut)[[1]][1]`. L'écart inter-quartile est une mesure de dispersion. Ce type de mesure sera décrit ultérieurement. 

La barre à l'intérieur de la boîte représente la **médiane** (valant `r quartiles(data3.7_brut)[[1]][2]` dans l'exemple de la `r table3.7`). La position de la médiane à l'intérieur de la boîte indique le degré de symétrie ou d'asymétrie de la portion centrale de la distribution.

```{r moustaches, echo=FALSE}
moustaches_data3.7=1.5*(quartiles(data3.7_brut)[[1]][3]-quartiles(data3.7_brut)[[1]][1])
msup_data3.7=quartiles(data3.7_brut)[[1]][3]+ moustaches_data3.7
minf_data3.7=quartiles(data3.7_brut)[[1]][1]- moustaches_data3.7
```

Les **moustaches** (les lignes qui sortent de part et d'autre de la boîte centrale)  dépendent des **barrières** qui sont situées à une distance de 1,5 l'écart inter-quartile (soit la longueur de la boîte) de part et d'autre de la boîte. Etant donné que dans l'exemple développé, l'écart inter-quartile vaut `r quartiles(data3.7_brut)[[1]][3]-quartiles(data3.7_brut)[[1]][1]`, les barrières s'éloigneront de $1,5 \times `r quartiles(data3.7_brut)[[1]][3]-quartiles(data3.7_brut)[[1]][1]` = `r moustaches_data3.7`$ points des extrémités de la boîte centrale. En conséquence:  

* la barrière supérieure vaut `r quartiles(data3.7_brut)[[1]][3]`+`r moustaches_data3.7`= `r msup_data3.7`

* la barrière inférieure vaut `r quartiles(data3.7_brut)[[1]][1]`-`r moustaches_data3.7`= `r minf_data3.7`

Cependant, une fois les barrières déterminées, il se peut qu'elles ne correspondent à aucune valeur existante de la distribution. Nous allons donc observer les valeurs adjacentes à l'intérieur des barrières. Les scores de la variables étudiée qui se situent à l'**intérieur** des barrières varient de `r min(data3.7_brut[data3.7_brut>=minf_data3.7&data3.7_brut<=msup_data3.7])` à `r max(data3.7_brut[data3.7_brut>=minf_data3.7&data3.7_brut<=msup_data3.7])`. Donc, la valeur adjacente inférieure est de `r min(data3.7_brut[data3.7_brut>=minf_data3.7&data3.7_brut<=msup_data3.7])` et la valeur adjacente supérieure est de `r max(data3.7_brut[data3.7_brut>=minf_data3.7&data3.7_brut<=msup_data3.7])`. C'est à ces valeurs adjacentes que correspondent les extrémités visibles des moustaches. 

Les **valeurs extrêmes** sont les points de part et d'autres des moustaches. Ce sont des valeurs qui sont supérieures à la barrière supérieure, ou inférieures à la barrière inférieure. Elles correspondent à des valeurs dont le score semble anormalement élevé ou bas, compte tenu de l'ensemble des observations étudiées. Ce serait par exemple le casde l'âge d'un individu ayant 51 ans, dans une étude consacrée à des adolescents. 

Le boxplot a pour avantage de permettre de détecter très rapidement ces valeurs extrêmes. Par ailleurs, elle permet de se représenter mentalement l'allure de la distribution: est-elle symétrique ou asymétrique?

### Variables quantitatives continues

#### Simplification des données: calcul de la médiane et des quartiles

##### Au départ de données brutes

Lorsque les données brutes sont fournies, le calcul de la médiane se réalise exactement de la même manière que pour les données quantitatives discrètes. Je vous renvoie donc à la section antérieure sur les variables quantitatives discrètes.

##### Au départ de tableaux de fréquences

Au début du chapitre, il a été vu que réaliser un tableau de fréquences de données continues implique préalablement de regrouper les valeurs par classe. Bien que ces regroupements constituent une perte d'information, il est malgré tout possible d'estimer la valeur de la médiane et des quantiles, en utilisant le **polygone des effectifs**.

Revenons une fois encore à l'exemple du PIB de la `r table3.2_freq` dont je reproduis ci-dessous le polygone des effectifs cumulés, pour votre convénience. Comme vous l'aurez compris, travailler sur le graphique dans lequel l'axe des y représente les fréquences absolues cumulées, ou dans celui sur lequel l'axe des y représente les fréquences relatives cumulées revient au même. Comme je trouve personnellement plus facile d'utiliser celui qui représente les fréquences relatives cumulées, j'opte pour ce choix.

```{r cumpolygon_PIB2, echo=FALSE}
h=hist(data3.2_brut$PIB_hab,breaks=nb_categ_PIB,plot=FALSE)
h$counts <- cumsum(h$counts) # replace the cell freq.s by cumulative freq.s
plot(h,col="lightgrey",cex.main=.8,main="Histogramme de PIB",ylab="Fréquence relative",cex.lab=1,xlab="PIB",las=2,cex.axis=.8,xlim=c(0,120000),ylim=c(0,length(data3.2_brut$PIB_hab)),yaxt="n") # plot a cumulative histogram of y
axis(2, at = seq(0,length(data3.2_brut$PIB_hab),5), labels = round(seq(0,length(data3.2_brut$PIB_hab),5)/length(data3.2_brut$PIB_hab),2),las = 2,cex.axis=.8)
segments(centre_PIB[1]-(amplitude),0,centre_PIB[1],abs_data3.2[1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
for(i in 1:(length(centre_PIB))){
segments(centre_PIB[i],abscum_data3.2[i],centre_PIB[i+1],abscum_data3.2[i+1],lwd=2)  
points(centre_PIB[i],abscum_data3.2[i],pch=16)
}
```

La fréquence cumulée de la valeur `r as.character(centre_PIB[1])` vaut `r round(rel_data3.2[1],2)` (soit une valeur inférieure à .5 ou 50%). La fréquence cumulée  de la valeur `r as.character(centre_PIB[2])` vaut `r round(rel_data3.2[2],2)` (soit une valeur supérieure à .5 ou 50%). Je sais donc que la valeur de la médiane se situera entre `r centre_PIB[1]` et `r centre_PIB[2]` et sera telle qu'il y ait exactement 50% des observations à sa gauche, et 50% des observations à sa droite. Une simple règle de trois permet de déterminer sa valeur: il y a une distance de `r round(relcum_data3.2[2],2)- round(relcum_data3.2[1],2)` unités entre `r round(relcum_data3.2[2],2)`% et `r round(relcum_data3.2[1],2)`%. Sur l'axe des x, cette distance est projetée sur une distance de `r as.character(centre_PIB[2]-centre_PIB[1])` (soit l'écart entre les deux centres de classes). 

Il y a une distance de `r 50 - round(relcum_data3.2[1],2)` unités entre 50% et `r round(relcum_data3.2[1],2)`%. Par règle de trois, on en déduit que cela correspond à une distance de $\frac{`r as.character(centre_PIB[2]-centre_PIB[1])`}{20} \times `r 50- round(relcum_data3.2[1],2)` = `r (centre_PIB[2]-centre_PIB[1])/(round(relcum_data3.2[2],2)- round(relcum_data3.2[1],2))*(50- round(relcum_data3.2[1],2))`$ sur l'axe des x. Par conséquent, la médiane sera estimée à `r as.character(centre_PIB[1])`+`r (centre_PIB[2]-centre_PIB[1])/(round(relcum_data3.2[2],2)- round(relcum_data3.2[1],2))*(50- round(relcum_data3.2[1],2))` = `r as.character(centre_PIB[1]+(centre_PIB[2]-centre_PIB[1])/(round(relcum_data3.2[2],2)- round(relcum_data3.2[1],2))*(50- round(relcum_data3.2[1],2)))`. 

En suivant un raisonnement identique, l'on estime les valeurs du premier et troisième quartile... A FAIRE!

#### Réalisation graphique: le boxplot

A nouveau, le boxplot se déroulement de manière identique à ce qui a été vu dans le chapitre sur les variables quantitatives discrètes. Seule la manière de déterminer la médiane ainsi que les premier et troisième quartile diffère.   

###### Pagebreak  

# Chapitre 4: exploration algébrique des données à une dimension

Jusqu'à présent, nous n'avons envisagé la présentation d'une distribution statistique que sous forme graphique. Cependant, plusieurs caractéristiques sont importantes à déterminer algébriquement. On peut distinguer trois grandes catégories d'indicateurs algébriques essentiels:

1) Les mesures de **tendance centrale**:mesure qui permet de représenter au mieux un ensemble données par une valeur unique 
2) Les mesures de **dispersion**: mesure du degré auquel les données s'éloignent de la tendance centrale
3) Les mesures d'**asymétrie** et d'**aplatissement **: mesures liées à la forme de la distribution. 

## Mesures de tendance centrale

Parmi elles, se trouvent la moyenne, le mode et la médiane. En tant que quantile particulier, la médiane a déjà été décrite dans le `r chap_graph`. Pour illustrer le mode et la moyenne, reprenons l'exemple de l'ancienneté de la  `r table1_brut`. Pour votre facilité, je reproduis ci-dessous ces données, sous forme de données brutes (a) et de tableau de fréquences (b):

```{r echo=FALSE, results='asis'}
data4.1_brut=data.frame(data1_brut$id,data1_brut$anciennete)
colnames(data4.1_brut)=c("id","anciennete")
data4.1_brut_transposed=t(data4.1_brut)
rownames(data4.1_brut_transposed)=c("i","$Y_i$")
```

`r table4.1`  
_Extrait de la `r table1_brut` : ancienneté de `r length(data1_brut$id)` employés_  

_**(a) Données brutes**_
```{r echo=FALSE, results='asis'}
kable(data4.1_brut_transposed,col.names=rep("",length(data4.1_brut$id)),align="c")
```

```{r echo=FALSE, results='asis'}

freq_data4.1=table(data4.1_brut$anciennete,deparse.level=2)
names_data4.1=min(as.numeric(names(freq_data4.1))):as.numeric(names(freq_data4.1[length(freq_data4.1)]))
categ=names_data4.1
abs_data4.1=tabulate(data4.1_brut$anciennete)[min(data4.1_brut$anciennete):length(tabulate(data4.1_brut$anciennete))]

nb_col=1

# Découpe en vue de l'affichage graphique
Num_gr=matrix(0,length(names_data4.1)/nb_col,nb_col) 
Anc_gr=matrix(0,length(names_data4.1)/nb_col,nb_col)
data4.1_splitted=matrix(0,length(names_data4.1)/nb_col,nb_col*2)

for(j in 1:nb_col){
Num_gr[,j]=names_data4.1[((length(names_data4.1)/nb_col)*(j-1)+1):(length(names_data4.1)/nb_col*j)]
Anc_gr[,j]=abs_data4.1[((length(names_data4.1)/nb_col)*(j-1)+1):(length(names_data4.1)/nb_col*j)]
data4.1_splitted[,((3*j)-(j+1)):(2*j)]=cbind(Num_gr[,j],Anc_gr[,j])}
colnames(data4.1_splitted)=c("$y_j$","$n_j$")
```
 
_**(b) Tableau de fréquence**_  
```{r echo=FALSE, results='asis'}
kable(t(data4.1_splitted),rownames="",align="c",)
```

Pour rappel, on utilise une lettre majuscule indicée par _i_ pour décrire les données brutes ($Y_i$ = la valeur du ième sujet sur la variable Y), et une lettre minuscule indicée par _j_ lorsqu'on parle des valeurs de la variable Y dans un tableau de fréquence ($y_j$ = la jème valeur que peut prenre la variable Y).

### Mode

```{r echo=FALSE, results='asis'}
freq_data4.2=table(data1_brut$age,deparse.level=2)
names_data4.2=min(as.numeric(names(freq_data4.2))):as.numeric(names(freq_data4.2[length(freq_data4.2)]))
categ=names_data4.2
abs_data4.2=tabulate(data1_brut$age)[min(data1_brut$age):length(tabulate(data1_brut$age))]

nb_col2=1

# Découpe en vue de l'affichage graphique
Num_gr=matrix(0,length(names_data4.2)/nb_col2,nb_col2) 
Age_gr=matrix(0,length(names_data4.2)/nb_col2,nb_col2)
data4.2_splitted=matrix(0,length(names_data4.2)/nb_col2,nb_col2*2)

for(j in 1:nb_col2){
Num_gr[,j]=names_data4.2[((length(names_data4.2)/nb_col2)*(j-1)+1):(length(names_data4.2)/nb_col2*j)]
Age_gr[,j]=abs_data4.2[((length(names_data4.2)/nb_col2)*(j-1)+1):(length(names_data4.2)/nb_col2*j)]
data4.2_splitted[,((3*j)-(j+1)):(2*j)]=cbind(Num_gr[,j],Age_gr[,j])
colnames(data4.2_splitted)=rep(c("$x_j$","$n_j$"),nb_col2)}
```

Le mode correspond à la classe la plus représentée. Dans la `r table4.1` (a ou b), vous consaterez aisément que `r names_data4.1[which(abs_data4.1 == max(abs_data4.1))]` est le nombre d'années d'ancienneté le plus représenté, puisque 
`r max(abs_data4.1)` employés ont cette ancienneté. C'est donc le mode de la distribution.   

Remarquez qu'une distribution peut être multimodale. Par exemple, dans la série de la table `r table4.2`, il y a deux valeurs plus représentées que les autres: ce sont les valeurs  `r names_data4.2[which(abs_data4.2 == max(abs_data4.2))][1]` et `r names_data4.2[which(abs_data4.2 == max(abs_data4.2))][2]`. Quand il y a deux modes, on parle plus spécifiquement de distribution bimodale.
 
`r table4.2`  
_Extrait de la `r table1_brut` : âge `r length(data1_brut$id)` employés_  

_**(a) Données brutes**_
```{r echo=FALSE, results='asis'}
id=1:length(data1_brut$id)
data4.2_brut=cbind(data1_brut$id,data1_brut$age)
colnames(data4.2_brut)=c("i","$X_i$")
```

```{r echo=FALSE, results='asis'}
kable(t(data4.2_brut),col.names=rep("",length(data1_brut$id)),rownames="",align="c")
```

_**(b) Tableau de fréquence**_  
```{r echo=FALSE, results='asis'}
kable(t(data4.1_splitted),rownames="",align="c",)
```

Le grand avantage du mode est d'être insensible aux valeurs extrêmes. Imaginons que par exemple, j'ai encodé par erreur que l'ancienneté du sujet 14, $Y_14$= `r as.numeric(paste0(data4.1_brut$anciennete[14],0))` ou même `r as.numeric(paste0(data4.1_brut$anciennete[14],0,0))` au lieu de `r as.numeric(data4.1_brut$anciennete[14])`. Le mode resterait identique, ce serait  `r names_data4.1[which(abs_data4.1 == max(abs_data4.1))]`. Ca ne sera pas le cas pour la moyenne, comme nous le verrons plus tard. En revanche, le grand désavantage du mode est qu'il ne dépend que de la ou des quelques valeurs les plus représentées et est totalement insensible au reste de la distribution (même des valeurs qui n'ont rien d'aberrantes). 

### Moyenne

Lorsqu'on parle de moyenne statistique, on entend toujours (à notre niveau) la moyenne arithmétique. Si l'on part des données brutes, la moyenne arithmétique peut se calculer en prenant la somme des valeurs de la variable dont on veut calculer la moyenne, et en divisant cette somme par le nombre de valeurs qui constituent cette somme. La moyenne arithématique se symbolise par une barre horizontale placée au dessus de la lettre majuscule qui représente la variable étudiée. Dans les formules, nous utilisons généralement la lettre "X", mais cette lettre peut être remplacée par n'importe quelle autre lettre représentant une variable). Pour reprendre l'exemple de la `r table4.1`,  dans la mesure où l'ancienneté est représentée par la lettre Y, la moyenne de cette variable se notera $\bar{Y}$.

**Formule de la moyenne arithmétique calculée à partir des données brutes de l'ancienneté **

$$ \bar{X} = \frac {\sum_{i=1}^n X_i}{n} $$
$$ \bar{Y} = \frac {`r chaine(data4.1_brut$anciennete)`}{`r length(data4.1_brut$anciennete)`} = \frac {`r sum(data4.1_brut$anciennete)`}{`r length(data4.1_brut$anciennete)`} = `r round(mean(data4.1_brut$anciennete),2)` $$

Lorsqu'il y a un grand nombre de données, appliquer cette formule manuellement devient relativement ardu (imaginez que vous deviez additionner le score de 300 participants, et diviser ensuite cette somme par 300, cela devient long!). Dans ce cas, on peut utiliser les données présentées sous forme de distribution de fréquences, et calculer la moyenne, soit sur base des fréquences absolues, soit sur base des fréquences relatives. 

**Formule de la moyenne arithmétique calculée à partir d'un tableau de fréquence**

_**En utilisant les valeurs $x_j$ et les fréquences absolues**_
$$ \bar{X} = \frac {\sum_{j=1}^k n_j \times x_j}{n} $$
_**En utilisant les valeurs $x_j$ et les fréquences relatives**_
$$ \bar{X} = \sum_{j=1}^k f_j \times x_j $$

Appliquées à l'ancienneté fournie dans la table `r table4.1`, on obtient ceci: 

```{r echo=FALSE, results='asis'}
chaine_abs1=NULL
chaine_abs2=NULL
chaine_rel=NULL
rep_abs1=rep(0,length(abs_data4.1))
rep_abs2=rep(0,length(abs_data4.1))
rep_rel=rep(0,length(abs_data4.1))
rel_anc=abs_data4.1/length(data4.1_brut$anciennete)

for (i in 1:length(abs_data4.1)){
if (i !=length(abs_data4.1)){
rep_abs1[i]=paste(abs_data4.1[i],"x",names_data4.1[i],"+")
rep_abs2[i]=paste(abs_data4.1[i]*names_data4.1[i],"+")
rep_rel[i]=paste(round(rel_anc[i],2),"x",names_data4.1[i],"+")
}else if (i==length(abs_data4.1)){
rep_abs1[i]=paste(abs_data4.1[i],"x",names_data4.1[i])
rep_abs2[i]=abs_data4.1[i]*names_data4.1[i]
rep_rel[i]=paste(round(rel_anc[i],2),"x",names_data4.1[i])}

chaine_abs1=paste(chaine_abs1,rep_abs1[i])
chaine_abs2=paste(chaine_abs2,rep_abs2[i])
chaine_rel=paste(chaine_rel,rep_rel[i])
}
```

_**En utilisant les valeurs $x_j$ et les fréquences absolues**_
$$ \bar{Y} = \frac {`r chaine_abs1`}{`r length(data4.1_brut$anciennete)`}$$    
$$ = \frac {`r chaine_abs2`}{`r length(data4.1_brut$anciennete)`} = \frac {`r sum(names_data4.1*abs_data4.1)`}{`r length(data4.1_brut$anciennete)`} = `r round(mean(data4.1_brut$anciennete),2)` $$ 

_**En utilisant les valeurs $x_j$ et les fréquences relatives**_
$$ \bar{Y} = `r chaine_rel` = `r round(mean(data4.1_brut$anciennete),2)` $$ 

Bien entendu, les trois formules donnent une valeur identique de la moyenne, et heureusement!Il s'agit de trois manières différentes de calculer la même chose. 

```{r echo=FALSE, results='asis'}
anciennete2=data4.1_brut$anciennete # remplacement de 10 en 100
anciennete3=data4.1_brut$anciennete # remplacement de 10 en 1000
anciennete2[14]=as.numeric(paste0(anciennete2[14],0))
anciennete3[14]=as.numeric(paste0(anciennete3[14],0,0))
```
La moyenne présente plusieurs inconvénients. 

Premièrement, elle est très sensible aux valeurs aberrantes. Rappelez-vous que lorsque nous avons envisagé le mode, ce dernier était parfaitement insensible à un $Y_14$= `r as.numeric(paste0(data4.1_brut$anciennete[14],0))` ou `r as.numeric(paste0(data4.1_brut$anciennete[14],0,0))` qui remplacerait le `r as.numeric(data4.1_brut$anciennete[14])` dans les données brutes. Dans le cas de la moyenne, ce n'est plus le cas du tout. En effet, le remplacement du `r data4.1_brut$anciennete[14]` en `r as.numeric(paste0(data4.1_brut$anciennete[14],0))` fait passer la moyenne de `r round(mean(data4.1_brut$anciennete),2)` à  `r round(mean(anciennete2),2)` et le remplacement du `r data4.1_brut$anciennete[14]` en `r as.numeric(paste0(data4.1_brut$anciennete[14],0,0))` fait passer la moyenne à `r round(mean(anciennete3),2)`. Constatez que ces valeurs ne permettent pas du tout de représenter correctement l'ancienneté habituelle des employés (et heureusement!). C'est donc un désavantage sérieux qui nous oblige à être attentifs aux valeurs aberrantes. 

Deuxièmement, si l'on travaille avec des distributions asymétriques ou multimodales, la moyenne ne les représentera pas correctement.Imaginons une classe constituée de 30 étudiants dans laquelle la moitié a obtenu 0/10 à une interrogation, et où l'autre moitié a obtenu 10/10 (dans cette classe, la distribution est bimodale). La moyenne vaudra exactement 5/10. Pourtant, aucun étudiant n'a obtenu une note proche de 5/10.

Nous voyons bien à quel point il est important d'étudier la forme d'une distribution avant de déterminer des valeurs algébriques. 

## Mesures de dispersion

Une mesure de dispersion sert à quantifier à quel points les données ont tendance à s'étaler, se disperser autour de la tendance centrale. Ces mesures sont importantes car la mesure de tendance centrale résumera d'autant mieux les données que la dispersion des données autour de celle-ci sera faible. 

Pour l'illustrer, considérons les deux séries suivantes qui représentent (fictivement) les notes (sur 10) obtenues par 10 étudiants en mathématiques et en français.

```{r echo=FALSE, results='asis'} 
id = 1:10
math=c(8.5,4,4.5,6,3.5,6.5,7,5,7.5,7.5)
francais=c(7,6,5.5,6,4.5,6,6.5,5.5,6.5,6.5)
notes_brut=data.frame(id,math,francais)
```

`r notes_compta`  
_Notes (/10) obtenues par `r length(id)` des étudiants en mathématiques et en français_
```{r echo=FALSE, results='asis'}
kable(notes_brut,format="markdown",align="c",col.names=c("$i$","math($X_i)$","francais($Z_i)$")) 
```

Je vous laisse le soin de calculer la moyenne de ces deux séries et de constater que dans les deux cours, elle vaut exactement `r mean(math)`. Par contre, la dispersion des données autour de cette tendance centrale n'est pas la même pour les deux cours. 

Dans la `r Figure4.1`, les notes de chaque cours sont représentées. La ligne horizontale représente la moyenne. Les points au dessus de la ligne horizontale sont les  notes supérieures à la moyenne et les points en dessous de cette ligne sont les notes inférieures à la moyenne. On observe graphiquement que les notes s'éloignent plus de la moyenne pour le cours de mathématiques que pour le cours de français. Cela se confirme algébriquement: en français, les notes s'éloignent au maximum de `r max(notes_brut$francais-mean(notes_brut$francais))`points de la moyenne alors qu'en mathématiques, les notes peuvent s'éloigner jusqu'à `r max(notes_brut$math-mean(notes_brut$math))`points de la moyenne. Autrement dit, la dispersion des notes est plus forte pour le cours de mathématiques que pour le cours de français.

`r Figure4.1`  
_Etalement des notes de la `r notes_compta` autour de la moyenne (`r mean(notes_brut$math)`)_
```{r echo=FALSE, results='asis'}
par(mfrow=c(1,2))
plot(id,notes_brut$math,xaxt="n",bty="n",ylim=c(mean(notes_brut$math)-3,mean(notes_brut$math)+3),pch=16,col="black",cex=1.5,ylab="notes",main="Mathématiques",xlab=" ")
abline(h=mean(notes_brut$math),lty=2)
segments(id,mean(notes_brut$math),id,notes_brut$math,lty=3)

plot(id,notes_brut$francais,xaxt="n",bty="n",ylim=c(mean(notes_brut$francais)-3,mean(notes_brut$francais)+3),pch=16,col="black",cex=1.5,ylab="notes",main="Francais",,xlab=" ")
abline(h=mean(notes_brut$francais),lty=2)
segments(id,mean(notes_brut$francais),id,notes_brut$francais,lty=3)
```

Quantifier la dispersion des données peut se faire de plusieurs manières plus ou moins sophistiquées. Nous en ciblerons quatre:      
*L'étendue 
*L'écart interquartile
*L'écart moyen absolu
*La variance et l'écart-type

### Etendue

L'étendue est une mesure très élémentaire qui consiste à mesurer la différence entre la plus grande et la plus petite des valeurs observées. 
$$ etendue = max(X_i) - min (X_i)$$
Dans l'exemple des notes de cours de la `r notes_compta`, on trouve que l'étendue des notes en français vaut `r max(notes_brut$francais)-min(notes_brut$francais)`, et que celle des notes en mathématiques vaut `r max(notes_brut$math)-min(notes_brut$math)`. 

L'avantage de cette mesure est qu'elle est très simple à calculer. En contre-partie, elle présente des inconvénients importants. Premièrement, elle ne dépend que deux valeurs de la série (la plus grande et la plus petite) et ne prend pas du tout en compte toutes les autres valeurs. Deuxièmement, elle est très sensible aux valeurs extrêmes: il suffit qu'un seul sujet ait un score atypique pour que l'étendue augmente considérablement. Vous pouvez donc regarder cet indicateur lorsque vous abordez en un coup d'oeil la distribution de vos résultats, mais n'accordez aucun crédit à cet indicateur dès lors que vous entrez dans une analyse plus fine.

### Ecart interquartile

L'écart interquartile consiste à mesurer l'écart entre le troisième et le premier quartile (c'est la boite centrale de la boîte à moustaches décrite précédemment, soit les 50% des données centrales de la distribution). 
$$EIQ = Q_3 - Q_1$$

```{r echo=FALSE, results='asis'} 
math_Q=quartiles(notes_brut$math)
francais_Q=quartiles(notes_brut$francais)
```

Dans l'exemple des notes de cours de la `r notes_compta`, on trouve que l'écart interquartile des notes en français vaut `r francais_Q$quartiles[3]-francais_Q$quartiles[1]`, et que celui des notes en mathématiques vaut `r math_Q$quartiles[3]-math_Q$quartiles[1]`. 

Contrairement à l'étendue des données, l'écart interquartile présente l'avantage de ne pas être sensible aux valeurs extrêmes.

### Ecart moyen absolu (EMA)

Un autre moyen de calculer la dispersion est de mesurer les écarts par rapport à la moyenne (E), tel que fait à la deuxième colonne de la `r notes_ecart_math` et de la `r notes_ecart_francais`. 

$$E_i=X_i-\bar{X_i}$$


```{r echo=FALSE, results='asis'} 
id = 1:10
math_ecarts=notes_brut$math-mean(notes_brut$math)
math_EMA=data.frame(id,math,math_ecarts)

francais_ecarts=notes_brut$francais-mean(notes_brut$francais)
francais_EMA=data.frame(id,francais,francais_ecarts)
```

`r notes_ecart_math`  
_Notes en % obtenues par `r length(id)` des étudiants en mathématiques_
```{r echo=FALSE, results='asis'}
kable(math_EMA,format="markdown",align="c",col.names=c("$i$","$math(X_i)$","$E_i$")) 
```

`r notes_ecart_francais`  
_Notes en % obtenues par `r length(id)` des étudiants en francais_
```{r echo=FALSE, results='asis'}
kable(francais_EMA,format="markdown",align="c",col.names=c("$i$","$francais(Z_i)$","$E_i$"))
```

Une information intéressante, en vue de déterminer la dispersion, serait de pouvoir estimer l'écart moyen des observations par rapport à la moyenne. Cependant, nous nous retrouvons confrontés à un problème: calculer la moyenne des écarts consisterait à additionner tous ces écarts, et à diviser la somme des écarts par le nombre de termes additionnés. Or, étant donné que certains écarts sont supérieurs à 0 et que d'autres sont inférieurs à 0, lorsque l'on tente d'additionner tous les écarts par rapport à la moyenne, on se retrouve invariablement avec un résultat nul:

Pour le cours de math: $\sum_{i=1}^n E_i=`r chaine(math_ecarts)`=`r sum(math_ecarts)`$  

Pour le cours de français: $\sum_{i=1}^n E_i=`r chaine(francais_ecarts)`=`r sum(francais_ecarts)`$

Il existe deux moyens de contourner ce problème. Soit prendre la valeur absolue de chaque écart, soit élever toutes les erreurs au carré (et donc ne plus se soucier du signe, puisqu'un chiffre négatif élevé au carré devient positif). 

La première stratégie est celle choisie lorqu'on calcule l'EMA. l'EMA est une estimation de l'écart à la moyenne, en moyenne, par sujet. Il se calcule comme suit:
$$EMA=\frac {\sum_{i=1}^n |X_i-\bar{X}|}{n}$$

Appliquée aux notes de mathématiques de la `r notes_compta`, cela donne $$\frac{`r chaine(abs(math_ecarts))`}{`r length(math_ecarts)`}=`r sum(abs(math_ecarts))/length(math_ecarts)`$$. 

Cela revient à dire que les notes s'écartent en moyenne de `r sum(abs(math_ecarts))/length(math_ecarts)` points par rapport à la moyenne de `r mean(notes_brut$math)`.

Appliquée aux notes de français de la `r notes_compta`, cela donne $$\frac{`r chaine(abs(francais_ecarts))`}{`r length(francais_ecarts)`}=`r sum(abs(francais_ecarts))/length(francais_ecarts)`$$. 

Cela revient à dire que les notes s'écartent en moyenne de `r sum(abs(francais_ecarts))/length(francais)` points par rapport à la moyenne de `r mean(notes_brut$francais)`.

Bien que cette mesure est une excellente façon de se représenter la dispersion, elle est totalement supplantée par l'écar-type. Cela provient du fait que l'écart-type est dérivé de la variance qui jouit de propriétés mathématiques qui lui font jouer un rôle central en statistique théorique.

### Variance et ecart-type

Nous avons envisagé de résoudre le problème de la somme nulle des erreurs (des écarts à la moyenne) en élevant chaque écart au carré. C'est la stratégie utilisée lorsque l'on calcule la variance des données. La variance consiste donc à calculer la valeur moyenne du carré des écarts entre les données observées et leur moyenne. Elle se calcule comme suit:

$$Variance=\frac {\sum_{i=1}^n (X_i-\bar{X})^2}{n}$$
Appliquée aux notes de mathématiques de la `r notes_compta`, cela donne: $$\frac{`r chaine(math_ecarts^2)`}{`r length(math_ecarts)`}=`r sum(math_ecarts^2)/length(math_ecarts)`$$. 

Il y a cependant un petit problème: la moyenne de la somme des carrés des écarts (SCE) est un chiffre dont l'unité est le carré de l'unité des données étudiées. Trouver une variance de `r sum(math_ecarts^2)/length(math_ecarts)` revient à dire que les notes s'écartent en moyenne de `r sum(math_ecarts^2)/length(math_ecarts)` "points au carré" de la moyenne de `r mean(notes_brut$math)`. C'est donc très difficilement interprétable. Pour résoudre ce problème, on va ramener cet indicateur à la même unité que l'unité de la moyenne, c'est-à-dire les "points". Pour ce faire, il suffit d'extraire la racine carrée de la variance: $\sqrt{`r sum(math_ecarts^2)/length(math_ecarts)`}=`r round(sqrt(sum(math_ecarts^2)/length(math_ecarts)),2)`$. Cette mesure constitue **l'écart-type**. Nous pouvons maintenant dire que les sujets s'écartent en moyenne de `r round(sqrt(sum(math_ecarts^2)/length(math_ecarts)),2)` points autour de la moyenne de `r mean(notes_brut$math)`.  

Appliquée aux notes de français de la `r notes_compta`, cela donne: $$\frac{`r chaine(francais_ecarts^2)`}{`r length(francais_ecarts)`}=`r sum(francais_ecarts^2)/length(francais_ecarts)`$$. 

L'écart-type, soit la racine carrée de la variance vaut $\sqrt{`r sum(francais_ecarts^2)/length(francais_ecarts)`}=`r round(sqrt(sum(francais_ecarts^2)/length(francais_ecarts)),2)`$. Nous pouvons maintenant dire que les sujets s'écartent en moyenne de `r round(sqrt(sum(francais_ecarts^2)/length(francais_ecarts)),2)` points autour de la moyenne de `r mean(notes_brut$francais)`.  

Remarquez que la mesure de l'écart-type est un petit peu différente de celle que nous obtenions en utilisant l'EMA (Pour le cours de math, écart-type = `r round(sqrt(sum(math_ecarts^2)/length(math_ecarts)),2)` vs. EMA = `r sum(abs(math_ecarts))/length(math_ecarts)`; pour le cours de français, écart-type =  `r round(sqrt(sum(francais_ecarts^2)/length(francais_ecarts)),2)` vs. EMA = `r sum(abs(francais_ecarts))/length(francais_ecarts)`). L'écart-type est un petit peu plus conservateur, c'est-à-dire qu'il surestime un petit peu l'erreur par rapport à l'EMA. 

La variance et l'écart-type souffrent de la même limite que la moyenne: ils sont tous les deux sensibles aux valeurs abberrantes. Cette sensibilité est accentuée par le fait que l'on élève les écarts au carré!

## Mesures d'asymétrie et d'aplatissement

Les distributions statistiques peuvent avoir plusieurs formes. Nous avons déjà envisagé la différence entre les distributions unimodales et bimodales. Il est également possible de distinguer les formes des distributions sur base d'autres paramètres, tels que l'**asymétrie** et l'**aplatissement**. 

### Mesure d'asymétrie

Une distribution symétrique est telle que la moyenne et la médiane sont confondues. Sur la partie centrale de la `r Figure4.2`, on observe graphiquement que la première moitié de la distribution partie à gauche de la médiane) est la parfaite symétrie orthogonale de la deuxième moitié de la distribution (la partie à droite de la médiane).

`r Figure4.2`  
_Exemple de distributions à asymétrie négative (gauche) symétrique (milieu)  et à asymétrie positive (droite)_
```{r echo=FALSE, results='asis'}
n=1000000
A=1-rchisq(n,df=2)
B=rnorm(n)
C=rchisq(n,df=2)
par(mfrow=c(1,3),mar=c(8,2,8,2))
plot(density(A),main="asymétrie négative",xlab="")
plot(density(B),main="symétrie",xlab="")
plot(density(C),main="asymétrie positive",xlab="")
```
 
Une distribution peut également présenter une asymétrie négative (si la queue de la distribution tend vers les valeurs négatives de l'axe x; extrêmité gauche de la `r Figure4.2`) ou une asymétrie positive (si la queue de la distribution tend vers les valeurs positives de l'axe x; extrêmité droite de la `r Figure4.2`).

Bien qu'il existe plusieurs mesures d'asymétrie, la plus courante est celle proposée par Pearson, dont voici la formule:

$$G1=\frac{M_3}{S^3}$$
$$avec,M_3=\frac{\sum_{i=1}^n(X_i-\bar{X})^3}{n}$$
$$S=\sqrt{\frac{\sum_{i=1}^n((X_i-\bar{X})^2)}{n}}$$
Il n'est pas important que vous puissiez calculer cela manuellement. Par contre, vous devez pouvoir interpréter sa valeur:      
*Lorsque la distribution est parfaitement symétrique, le coefficient G1 vaut exactement 0.    
*Lorsque la distribution présente une asymétrie positive, le coefficient G1 a une valeur supérieure à 0.   
*Lorsque la distribution présente une asymétrie négative, le coefficient G1 a une valeur inférieure à 0.    

### Mesure d'aplatissement

Pour correctement comprendre la mesure d'aplatissement, il faut savoir que la distribution de référence est la distribution normale (ou courbe en cloche, représentée sur la `Figure 4.3`)

`r Figure4.3`  
_Distribution normale (appelée aussi courbe en cloche)_
```{r echo=FALSE, results='asis'}
n=1000000
A=rnorm(n)
plot(density(A),main="distribution normale",xlab="")
```

Une distribution dont le kurtosis vaut 0 est une distribution qui n'est ni plus plate ni plus pointue que la fameuse courbe en cloche que l'on appelle **distribution normale**. Une distribution dite **leptokurtique** est une distribution qui est plus pointue et avec des extrémités plus épaisses que la distribution normale. Enfin, une distribution dite **platikurtique** est une distribution aplatie, dont le centre et les extrémités sont moins fournies que ceux d'une distribution normale. 

`r Figure4.4`  
_Exemple de distributions d'aplatissement normale, leptokurtique et platikurtiques_
```{r echo=FALSE, results='asis'}
n=1000000
A=rnorm(n,sd=2)
B=rdoublex(n,lambda=2/sqrt(2))
C=runif(n,-3.465,3.465)
par(mfrow=c(1,1),mar=c(2,2,2,2),xpd=FALSE)
plot(density(A),main="distribution normale",xlab="",ylim=c(0,.32))
lines(density(B),lty=2)
lines(density(C),lty=3)
segments(-6,0,-4,0,lty=3)
segments(4,0,6,0,lty=3)
points(0,.25,cex=30,col="red")
points(-5,0,cex=13,col="blue")
points(5,0,cex=13,col="blue")
legend("topright", legend=c("normal", "leptokurtique","platikurtique"),lty=c(1,2,3), box.lty=1)
```

De nouveau, il existe plusieurs mesures de kurtosis. Voici celle proposée par Pearson:

$$G2=\frac{M_4}{S^4}-3$$
$$avec,M_4=\frac{\sum_{i=1}^n(X_i-\bar{X})^4}{n}$$
$$S=\sqrt{\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n}}$$

De même que pour la mesure d'asymétrie, il n'est pas important que vous puissiez calculer cela manuellement. Par contre, vous devez pouvoir interpréter sa valeur:     
*Lorsque la distribution a l'aplatissement d'une distribution normale, le coefficient G2 vaut exactement 0.  
*Lorsque la distribution est leptokurtique, le coefficient G2 a une valeur supérieure à 0.  
*Lorsque la distribution est platikurtique, le coefficient G2 a une valeur inférieure à 0.  

###### Pagebreak

# Chapitre 5: relation entre deux variables continues: le coefficient r de Pearson

Jusque là, nous avons passé beaucoup de temps à étudier les variables. Nous avons vu comment il était possible de les représenter graphiquement et aussi comment les étudier algébriquement. Tout cela relève de la statistique descriptive **univariée**. 

Le dernier objectif de ce cours est d'apprendre à observer comment deux variables peuvent varier **en même temps**, ou covarier. Cela relève de la statistique **bivariée**.

Revenons à l'exemple de la `r notes_ecart_math`  dans laquelle nous avions étudié la note de 10 étudiants à deux cours: le cours de mathématiques et le cours de français. 

Nous avons déjà précédemment étudié comment les notes de chacun de ces cours pouvaient varier individuellement d'un étudiant à l'autre. La question que nous nous posons maintenant est de savoir comment les notes de ces deux cours **covarient**. Autrement dit, existe-t-il un lien entre les notes aux deux cours ou encore, lorsque les notes à un cours augmentent, est-ce que celles de l'autre cours tendent à augmenter également? 

## Représentation graphique de la relation entre deux variables  

Une première manière d'appréhender la question est de se représenter graphiquement la situation. Lorsque l'on souhaite pouvoir étudier le lien entre deux variables, la meilleure représentation graphique consiste à tracer deux axes perpendiculaires, l'un représentant l'une des deux variables, l'autre représentant l'autre variable.

`r notes_compta`  
_Notes (/10) obtenues par `r length(id)` des étudiants en mathématiques et en français_
```{r echo=FALSE, results='asis'}
kable(notes_brut,format="markdown",align="c",col.names=c("$i$","math($X_i)$","francais($Z_i)$")) 
```

La `r correlation_graph_a` montre le graphe qui reprend les deux valeurs les unes par rapport aux autres. On y voit un nuage de points, chacun de ceux-ci ayant une double coordonnée: sur l'axe des x, et sur l'axe des y. Par exemple, le point entouré correspond à un individu ayant obtenu la note `r math[math==3.5]` au cours de mathématiques (coordonnée sur l'axe des x) et `r francais[math==3.5]` au cours de francais (coordonnée sur l'axe des y). 

`r correlation_graph_a`  
_Nuage de points représentant la relation entre la note à un cours de math et la note à un cours de français_  
```{r echo=FALSE, results='asis'}
par(mar=c(4,2,2,4))
plot(math,francais,pch=19,bty="n",xlim=c(0,10),ylim=c(0,10))
axis(1,at=seq(0,10,1),labels=seq(0,10,1))
axis(2,at=seq(0,10,1),labels=seq(0,10,1))
points(3.5,4.5,pch=21,cex=5)
segments(math[5],0,math[5],francais[5],lty=3)
segments(0,francais[5],math[5],francais[5],lty=3)
```

En observant le nuage de points dans son ensemble, il apparait que celui-ci n'est pas du tout aléatoire. Globalement, les individus ayant les meilleurs points en mathématiques semblent également avoir des points élevés en français. A contrario, ceux qui ont les moins bon points en mathématiques semblent avoir également de moins bons points en français. 

Pour mieux encore appréhender la relation qui existe entre deux variables, il est possible de tracer, au départ du nuage de points, une droite qui se rapproche le plus possible de l'ensemble des points (selon des règles très précises), que l'on appelera la **droite de régression**.   

`r correlation_graph_b`  
_Nuage de points représentant la relation entre la note à un cours de math et la note à un cours de français_  
```{r echo=FALSE, results='asis'}
par(mar=c(4,2,2,4))
plot(math,francais,pch=19,bty="n",xlim=c(0,10),ylim=c(0,10))
axis(1,at=seq(0,10,1),labels=seq(0,10,1))
axis(2,at=seq(0,10,1),labels=seq(0,10,1))
abline(a=lm(francais~math)$coefficients[1],b=lm(francais~math)$coefficients[2],lty=2)
```

On constate que l'ensemble des points semblent se rapprocher d'une droite de régression dont la pente est croissante. Ca sera le cas chaque fois que la relation linéaire entre deux variables est positive (càd quand le score des deux variables tend à varier dans le même sens). 

Imaginons à présent que nous souhaitions étudier la relation entre le cours de mathématiques et un cours de géographie suivi par les 10 mêmes étudiants. 
 
```{r echo=FALSE, results='asis'} 
id = 1:10
math=c(8.5,4,4.5,6,3.5,6.5,7,5.5,7.5,7.5)
geo=c(6,9,8.5,6.5,8.5,6.5,6,8,5.5,5.5)
notes_geo=data.frame(id,math,geo)
```

`r notes_compta2`  
_Notes (/10) obtenues par `r length(id)` des étudiants en mathématiques et en français_
```{r echo=FALSE, results='asis'}
kable(notes_geo,format="markdown",align="c",col.names=c("$i$","math($X_i)$","geographie($W_i)$")) 
```

Comme précédemment, l'étude du nuage de points et de la droite de régression associée aidera à mieux se représenter la relation entre les deux variables (voir `r `).

`r correlation_graph_geo`  
_Nuage de points représentant la relation entre la note à un cours de math et la note à un cours de géographie_  
```{r echo=FALSE, results='asis'}
par(mar=c(4,2,2,4))
plot(math,geo,pch=19,bty="n",xlim=c(0,10),ylim=c(0,10))
axis(1,at=seq(0,10,1),labels=seq(0,10,1))
axis(2,at=seq(0,10,1),labels=seq(0,10,1))
abline(a=lm(geo~math)$coefficients[1],b=lm(geo~math)$coefficients[2],lty=2)
```

Cette fois, les points semblent se rapprocher d'une droite de régression décroissante.Ca sera le cas chaque fois que la relation linéaire entre deux variables est négative (càd quand le score des deux variables tend à varier dans des sens opposés: lorsque score de mathématiques augmente, celui de géographique diminue, et réciproquement). 

Comme dernier cas de figure, imaginons que nous souhaitions étudier la relation entre le cours de géographie et d'anglais, toujours suivi par les 10 mêmes étudiants. 

```{r echo=FALSE, results='asis'} 
id = 1:10
geo=c(5.5,9,6,6,6.5,6.5,8,8.5,8.5,9)
anglais=c(10,5,6,7,5,8,8,9,8.5,9)
notes_geo=data.frame(id,geo,anglais)
```

`r notes_compta2`  
_Notes (/10) obtenues par `r length(id)` des étudiants en mathématiques et en français_
```{r echo=FALSE, results='asis'}
kable(notes_geo,format="markdown",align="c",col.names=c("$i$","geographie($W_i)$","anglais($V_i)$")) 
```

Encore une fois, l'étude du nuage de points et de la droite de régression associée aidera à mieux se représenter la relation entre les deux variables (voir `r `).

`r correlation_graph_agls`  
_Nuage de points représentant la relation entre la note à un cours de géographie et la note à un cours d'anglais_  
```{r echo=FALSE, results='asis'}
par(mar=c(4,8,2,4))
plot(geo,anglais,pch=19,bty="n",xlim=c(0,10),ylim=c(0,10))
axis(1,at=seq(0,10,1),labels=seq(0,10,1))
axis(2,at=seq(0,10,1),labels=seq(0,10,1))
abline(a=lm(geo~anglais)$coefficients[1],b=lm(geo~anglais)$coefficients[2],lty=2)
```

Contrairement au deux situations précédentes, les points ne se rapprochent ni d'une droite croissante, ni d'une droite décroissante. Il ne semble pas y avoir de relation entre les notes obtenues au cours de géographiques et celles obtenues au cours d'histoire. Par ailleurs, la droite de régression a une pente quasiment nulle. 

## Détermination algébrique de la relation entre deux variables  


###### Pagebreak
 
# Références

Labreuche, J. (2010). Les différents types de variables, leurs représentations graphiques et paramêtres descriptifs. Sang Thrombose Vaisseaux, 22(10), 536-543.

Syllabus de Christophe

Séminaire de Christian Ritter sur les graphiques

Support office


